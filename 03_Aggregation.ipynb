{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8b8417b-4eae-4569-8eb3-63acf557aef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83E\uDE99 Gold Layer: \n",
    "## Rolling Aggregations and more\n",
    "\n",
    "**Purpose:**  \n",
    "This notebook reads data from **Silver** tables, performs:\n",
    "- Idempotent upserts into **Gold** tables  \n",
    "\n",
    "It also updates the `pipeline_log` table for every execution — marking both **success** and **failure**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a146f99e-7b0b-4333-bed6-3d7684ebe6f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83E\uDDF0 Step 1: Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3c5fa77-c447-4840-948a-5e5461a149b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c35aea91-b503-4fda-be20-8ea3dfaf05fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ⚙️ Step 2: Configuration — Table Names and Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c706429-81a2-4193-af95-b571fe6375cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"env_catalog\"\n",
    "schema = \"env_data\"\n",
    "\n",
    "city_master_tbl = f\"{catalog}.{schema}.city_master\"\n",
    "pipeline_log_tbl = f\"{catalog}.{schema}.pipeline_log\"\n",
    "last_processed_tbl = f\"{catalog}.{schema}.source_last_processed_ts\"\n",
    "\n",
    "# bronze_weather_hourly_tbl   = f\"{catalog}.{schema}.bronze_weather_hourly\"\n",
    "# bronze_weather_daily_tbl    = f\"{catalog}.{schema}.bronze_weather_daily\"\n",
    "# bronze_air_hourly_tbl       = f\"{catalog}.{schema}.bronze_air_hourly\"\n",
    "# silver_weather_hourly_tbl = f\"{catalog}.{schema}.silver_weather_hourly\"\n",
    "# silver_air_hourly_tbl     = f\"{catalog}.{schema}.silver_air_hourly\"\n",
    "\n",
    "silver_weather_daily_tbl = f\"{catalog}.{schema}.silver_weather_daily\"\n",
    "silver_air_daily_tbl      = f\"{catalog}.{schema}.silver_air_daily\"\n",
    "\n",
    "gold_weather_daily_tbl = f\"{catalog}.{schema}.gold_weather_daily\"\n",
    "gold_air_daily_tbl = f\"{catalog}.{schema}.gold_air_daily\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f2fb5da-e4cd-4783-9528-3631274d0614",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83E\uDEB6 Created pipeline_log entry for run_id=DAILY_AGGREGATE_20251113_095439\n"
     ]
    }
   ],
   "source": [
    "pipeline_name = \"AGGREGATE\"\n",
    "# TODO: Determine if Manual run or Scheduled run\n",
    "triggered_by = \"Manual\"  # Can be Scheduled/Event driven if automated\n",
    "run_type = \"DAILY\"\n",
    "start_ts = datetime.now(timezone.utc)\n",
    "run_id = globals().get(\"run_id\", f\"{run_type}_{pipeline_name.upper()}_{start_ts.strftime('%Y%m%d_%H%M%S')}\")\n",
    "status = \"RUNNING\"\n",
    "remarks = \"Gold Aggregation layer job started\"\n",
    "\n",
    "# Initial pipeline log entry\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {pipeline_log_tbl} \n",
    "        (run_id, pipeline_name, run_type, start_time, status, triggered_by, remarks, created_ts)\n",
    "        VALUES ('{run_id}', '{pipeline_name}', '{run_type}', TIMESTAMP '{start_ts}', '{status}', '{triggered_by}', '{remarks}', current_timestamp())\n",
    "    \"\"\")\n",
    "    print(f\"\uD83E\uDEB6 Created pipeline_log entry for run_id={run_id}\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Could not log start in pipeline_log:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cdd7920-4284-426f-9e06-fc791d8cd358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83E\uDDE9 Step 3: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b269f6a-c419-427f-8f94-de089b11af08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def table_exists(name: str) -> bool:\n",
    "    \"\"\"Robust check for table existence (works across environments).\"\"\"\n",
    "    try:\n",
    "        # prefer catalog metadata check\n",
    "        if spark.catalog.tableExists(name):\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    # fallback: try to read metadata plan (no data read)\n",
    "    try:\n",
    "        spark.table(name).limit(0).count()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "    \n",
    "def ensure_table(name: str, schema_sdf, partition_col=None):\n",
    "    \"\"\"\n",
    "    Create a Delta table with the provided schema if it does not exist.\n",
    "    Uses schema_sdf.limit(0) to create structure only.\n",
    "    \"\"\"\n",
    "    if table_exists(name):\n",
    "        return\n",
    "    write_builder = schema_sdf.limit(0).write.format(\"delta\") .mode(\"overwrite\").option(\"overwriteSchema\", \"true\")\n",
    "    if partition_col:\n",
    "        write_builder = write_builder.partitionBy(partition_col)\n",
    "    write_builder.saveAsTable(name)\n",
    "    print(f\"Created table {name}\")\n",
    "\n",
    "def update_pipeline_log(status, remarks, records_processed=0, earliest_ts=None, latest_ts=None):\n",
    "    \"\"\"Update the pipeline_log for this run.\"\"\"\n",
    "    try:\n",
    "        earliest_expr = f\"TIMESTAMP '{earliest_ts}'\" if earliest_ts else \"NULL\"\n",
    "        latest_expr = f\"TIMESTAMP '{latest_ts}'\" if latest_ts else \"NULL\"\n",
    "        spark.sql(f\"\"\"\n",
    "            UPDATE {pipeline_log_tbl}\n",
    "            SET end_time = current_timestamp(),\n",
    "                status = '{status}',\n",
    "                records_processed = COALESCE(records_processed, 0) + {records_processed},\n",
    "                earliest_ts = COALESCE(earliest_ts, {earliest_expr}),\n",
    "                latest_ts = {latest_expr},\n",
    "                remarks = '{remarks}'\n",
    "            WHERE run_id = '{run_id}'\n",
    "        \"\"\")\n",
    "        print(f\"✅ pipeline_log updated: {status}\")\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Could not update pipeline_log:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a1ef4cb-c4be-41c4-8aad-01af0c266e05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83E\uDDFC Step 4: Data Cleansing and Data Enrichment Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "604f2270-9c2a-4bab-85c9-55412f5ed1b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def get_rolling_agg(df, metric_col):\n",
    "    \"\"\"\n",
    "    Compute rolling mean & max windows for metric_col, grouping the aggregation\n",
    "    ONLY by (city, obsv_date), and return the original DataFrame columns\n",
    "    augmented with the new aggregation columns and agg_ts.\n",
    "\n",
    "    Input:\n",
    "      df         : Spark DataFrame that must contain at least: city, obsv_date, and metric_col\n",
    "      metric_col : string name of metric column (e.g. \"aqi_value\", \"climate_index\")\n",
    "\n",
    "    Output:\n",
    "      DataFrame with all original df columns + columns:\n",
    "        mean_future7, max_future7,\n",
    "        mean_past7,   max_past7,\n",
    "        mean_past30,  max_past30,\n",
    "        mean_past90,  max_past90,\n",
    "        agg_ts\n",
    "    Notes:\n",
    "      - Aggregation windows are computed relative to the date part of obsv_date.\n",
    "      - If source has multiple rows per (city, obsv_date) those will join back unchanged.\n",
    "    \"\"\"\n",
    "\n",
    "    # keep original schema to re-attach later\n",
    "    orig_cols = df.columns\n",
    "\n",
    "    # prepare base: ensure needed fields and add obsv_date\n",
    "    base = df.select(*orig_cols).withColumn(\"obsv_date\", F.to_date(\"data_timestamp\"))\n",
    "    orig_ref = base \n",
    "    # aliases for windowing\n",
    "    win = base.alias(\"win\")\n",
    "    base_alias = base.alias(\"base\")\n",
    "\n",
    "    def agg_window(cond, mean_name, max_name):\n",
    "        \"\"\"\n",
    "        Aggregate mean & max for the given condition.\n",
    "        Grouping is ONLY on city and obsv_date\n",
    "        \"\"\"\n",
    "        return (\n",
    "            base_alias.join(win, cond, how=\"left\")\n",
    "            .groupBy(\"base.city\", \"base.obsv_date\")\n",
    "            .agg(\n",
    "                F.round(F.avg(F.col(f\"win.{metric_col}\")), 2).alias(mean_name),\n",
    "                F.round(F.max(F.col(f\"win.{metric_col}\")), 2).alias(max_name)\n",
    "            )\n",
    "            .withColumnRenamed(\"city\", \"city\")\n",
    "            .withColumnRenamed(\"obsv_date\", \"obsv_date\")\n",
    "        )\n",
    "\n",
    "    # Window conditions use obsv_date comparisons relative to the base row date\n",
    "    cond_fut7 = (\n",
    "        (F.col(\"win.city\") == F.col(\"base.city\")) &\n",
    "        (F.col(\"win.obsv_date\").between(F.col(\"base.obsv_date\"), F.date_add(F.col(\"base.obsv_date\"), 6)))\n",
    "    )\n",
    "    cond_past7 = (\n",
    "        (F.col(\"win.city\") == F.col(\"base.city\")) &\n",
    "        (F.col(\"win.obsv_date\").between(F.date_add(F.col(\"base.obsv_date\"), -7), F.date_add(F.col(\"base.obsv_date\"), -1)))\n",
    "    )\n",
    "    cond_past30 = (\n",
    "        (F.col(\"win.city\") == F.col(\"base.city\")) &\n",
    "        (F.col(\"win.obsv_date\").between(F.date_add(F.col(\"base.obsv_date\"), -30), F.date_add(F.col(\"base.obsv_date\"), -1)))\n",
    "    )\n",
    "    cond_past90 = (\n",
    "        (F.col(\"win.city\") == F.col(\"base.city\")) &\n",
    "        (F.col(\"win.obsv_date\").between(F.date_add(F.col(\"base.obsv_date\"), -90), F.date_add(F.col(\"base.obsv_date\"), -1)))\n",
    "    )\n",
    "\n",
    "    # compute aggregated frames (grouped by city & obsv_date)\n",
    "    fut7 = agg_window(cond_fut7, \"mean_future7\", \"max_future7\")\n",
    "    p7   = agg_window(cond_past7, \"mean_past7\", \"max_past7\")\n",
    "    p30  = agg_window(cond_past30, \"mean_past30\", \"max_past30\")\n",
    "    p90  = agg_window(cond_past90, \"mean_past90\", \"max_past90\")\n",
    "\n",
    "    # join all windows on city & obsv_date\n",
    "    joined = (\n",
    "        fut7.alias(\"f\")\n",
    "        .join(p7.alias(\"p7\"), [\"city\",\"obsv_date\"], \"left\")\n",
    "        .join(p30.alias(\"p30\"), [\"city\",\"obsv_date\"], \"left\")\n",
    "        .join(p90.alias(\"p90\"), [\"city\",\"obsv_date\"], \"left\")\n",
    "    )\n",
    "\n",
    "    # add agg_ts\n",
    "    joined = joined.withColumn(\"agg_ts\", F.current_timestamp())\n",
    "\n",
    "    # join aggregated results back to original dataframe on city & obsv_date\n",
    "    # keep all original columns and append agg columns\n",
    "    agg_cols = [c for c in joined.columns if c not in (\"city\",\"obsv_date\")]\n",
    "\n",
    "    # build a tidy agg side that contains join keys + agg cols\n",
    "    agg_df = joined.select(\"city\", \"obsv_date\", *agg_cols)\n",
    "\n",
    "    # explicit join, then explicitly select qualifed columns to avoid ambiguities\n",
    "    result = (\n",
    "        orig_ref.alias(\"orig\")\n",
    "        .join(agg_df.alias(\"agg\"),\n",
    "            on=[(F.col(\"orig.city\") == F.col(\"agg.city\")),\n",
    "                (F.col(\"orig.obsv_date\") == F.col(\"agg.obsv_date\"))],\n",
    "            how=\"left\")\n",
    "        .select(\n",
    "            *[F.col(f\"orig.{c}\") for c in orig_cols],        # all original columns (qualified)\n",
    "            *[F.col(f\"agg.{c}\")  for c in agg_cols]          # only new agg columns (qualified)\n",
    "        )\n",
    "    )\n",
    "    return result\n",
    "    \n",
    "def aqi_category_and_color(df):\n",
    "    \"\"\"\n",
    "    Adds AQI category and color columns to a DataFrame that already contains\n",
    "    rolling mean/max AQI columns (e.g., mean_future7, max_past30, etc.)\n",
    "\n",
    "    Columns added:\n",
    "      - aqi_<mean/max_window>_cat\n",
    "      - aqi_<mean/max_window>_color\n",
    "    \"\"\"\n",
    "\n",
    "    def cat_expr(c):\n",
    "        return (\n",
    "            F.when(F.col(c).isNull(), None)\n",
    "             .when(F.col(c) <= 50, \"Good\")\n",
    "             .when(F.col(c) <= 100, \"Moderate\")\n",
    "             .when(F.col(c) <= 150, \"Unhealthy for Sensitive Groups\")\n",
    "             .when(F.col(c) <= 200, \"Unhealthy\")\n",
    "             .when(F.col(c) <= 300, \"Very Unhealthy\")\n",
    "             .otherwise(\"Hazardous\")\n",
    "        )\n",
    "\n",
    "    def color_expr(c):\n",
    "        return (\n",
    "            F.when(F.col(c).isNull(), None)\n",
    "             .when(F.col(c) <= 50, \"Green\")\n",
    "             .when(F.col(c) <= 100, \"Yellow\")\n",
    "             .when(F.col(c) <= 150, \"Orange\")\n",
    "             .when(F.col(c) <= 200, \"Red\")\n",
    "             .when(F.col(c) <= 300, \"Purple\")\n",
    "             .otherwise(\"Maroon\")\n",
    "        )\n",
    "\n",
    "    # Apply to all rolling mean & max columns\n",
    "    mean_cols = [\"mean_future7\", \"mean_past7\", \"mean_past30\", \"mean_past90\"]\n",
    "    max_cols  = [\"max_future7\", \"max_past7\", \"max_past30\", \"max_past90\"]\n",
    "\n",
    "    for c in mean_cols:\n",
    "        df = (\n",
    "            df.withColumn(f\"aqi_{c}_cat\", cat_expr(c))\n",
    "              .withColumn(f\"aqi_{c}_color\", color_expr(c))\n",
    "        )\n",
    "\n",
    "    for c in max_cols:\n",
    "        df = (\n",
    "            df.withColumn(f\"aqi_{c}_cat\", cat_expr(c))\n",
    "              .withColumn(f\"aqi_{c}_color\", color_expr(c))\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "def climate_category_and_color(df):\n",
    "    \"\"\"\n",
    "    Adds Climate Index category and color columns to a DataFrame that already contains\n",
    "    rolling mean/max climate index columns (e.g., mean_future7, max_past30, etc.)\n",
    "\n",
    "    Columns added:\n",
    "      - clim_<mean/max_window>_cat\n",
    "      - clim_<mean/max_window>_color\n",
    "    \"\"\"\n",
    "\n",
    "    def cat_expr(c):\n",
    "        return (\n",
    "            F.when(F.col(c).isNull(), None)\n",
    "             .when(F.col(c) >= 81, \"Excellent\")\n",
    "             .when(F.col(c) >= 61, \"Good\")\n",
    "             .when(F.col(c) >= 41, \"Moderate\")\n",
    "             .when(F.col(c) >= 21, \"Fair\")\n",
    "             .otherwise(\"Poor\")\n",
    "        )\n",
    "\n",
    "    def color_expr(c):\n",
    "        return (\n",
    "            F.when(F.col(c).isNull(), None)\n",
    "             .when(F.col(c) >= 81, \"Green\")\n",
    "             .when(F.col(c) >= 61, \"LightGreen\")\n",
    "             .when(F.col(c) >= 41, \"Yellow\")\n",
    "             .when(F.col(c) >= 21, \"Orange\")\n",
    "             .otherwise(\"Red\")\n",
    "        )\n",
    "\n",
    "    mean_cols = [\"mean_future7\", \"mean_past7\", \"mean_past30\", \"mean_past90\"]\n",
    "    max_cols  = [\"max_future7\", \"max_past7\", \"max_past30\", \"max_past90\"]\n",
    "\n",
    "    for c in mean_cols:\n",
    "        df = (\n",
    "            df.withColumn(f\"clim_{c}_cat\", cat_expr(c))\n",
    "              .withColumn(f\"clim_{c}_color\", color_expr(c))\n",
    "        )\n",
    "\n",
    "    for c in max_cols:\n",
    "        df = (\n",
    "            df.withColumn(f\"clim_{c}_cat\", cat_expr(c))\n",
    "              .withColumn(f\"clim_{c}_color\", color_expr(c))\n",
    "        )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c55cb81-c230-4a90-9344-0f718c65be8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ⚡ Step 5: Function to capture last load/transform timestamp from source table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a181afb-7a48-4aac-bd19-8e962e03318e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_last_processed_load_ts(source_table: str):\n",
    "    \"\"\"Return last_processed_load_ts timestamp or None\"\"\"\n",
    "    if not table_exists(last_processed_tbl):\n",
    "        return None\n",
    "    try:\n",
    "        rows = spark.table(last_processed_tbl).filter(F.col(\"source_table\") == source_table).select(\"last_processed_load_ts\").limit(1).collect()\n",
    "        return rows[0][\"last_processed_load_ts\"] if rows else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def commit_last_processed_load_ts(source_table: str, last_ts):\n",
    "    \"\"\"Upsert last_processed_load_ts into meta table (run after successful merge)\"\"\"\n",
    "    if last_ts is None:\n",
    "        return\n",
    "    now_ts = datetime.now(timezone.utc)\n",
    "    staging = spark.createDataFrame([(source_table, last_ts, now_ts)], schema=\"source_table STRING, last_processed_load_ts TIMESTAMP, updated_ts TIMESTAMP\")\n",
    "    if not table_exists(last_processed_tbl):\n",
    "        staging.write.format(\"delta\").mode(\"overwrite\").saveAsTable(last_processed_tbl)\n",
    "        return\n",
    "    DeltaTable.forName(spark, last_processed_tbl).alias(\"t\").merge(\n",
    "        staging.alias(\"s\"),\n",
    "        \"t.source_table = s.source_table\"\n",
    "    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "    print(f\"Updated last processed timestamp for {source_table}: {last_ts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4031aa5d-53e4-4c8e-b58e-fddee5329b4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83D\uDD04 Step 6: Merge Incremental Data into Silver Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30336d04-4e4f-4dc7-a8e5-2c8c72cca8f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MERGE on (city, data_timestamp)\n",
    "def merge_into_gold(target_table: str, staging_sdf, merge_keys=(\"city\", \"data_timestamp\"), exclude_update_cols=None, partition_col=None):\n",
    "    \"\"\"\n",
    "    Idempotent upsert via Delta MERGE\n",
    "    Ensure target exists and MERGE the staging_sdf into it.\n",
    "    Returns rows_processed, min_data_ts, max_data_ts, max_load_ts (if transform_ts present)\n",
    "    \"\"\"\n",
    "    #print(\"Entered merge\")\n",
    "    if staging_sdf is None:\n",
    "        print(f\"Skipping {target_table} - no staging\")\n",
    "        return 0, None, None, None\n",
    "    \n",
    "    # Coerce data_timestamp/load_timestamp types if present\n",
    "    if \"data_timestamp\" in staging_sdf.columns:\n",
    "        staging_sdf = staging_sdf.withColumn(\"data_timestamp\", F.to_timestamp(\"data_timestamp\"))\n",
    "    # if \"load_timestamp\" in staging_sdf.columns:\n",
    "    #     staging_sdf = staging_sdf.withColumn(\"load_timestamp\", F.to_timestamp(\"load_timestamp\"))\n",
    "    if \"transform_ts\" in staging_sdf.columns:\n",
    "        staging_sdf = staging_sdf.withColumn(\"transform_ts\", F.to_timestamp(\"transform_ts\"))\n",
    "\n",
    "    # print(\"Before table creation\")\n",
    "    # ensure target exists\n",
    "    ensure_table(target_table, staging_sdf, partition_col=partition_col)\n",
    "    \n",
    "    # prepare temp view for staging\n",
    "    view = f\"temp_{target_table.split('.')[-1]}_{int(datetime.now().timestamp())}\"\n",
    "    staging_sdf.createOrReplaceTempView(view)\n",
    "\n",
    "    cols = staging_sdf.columns\n",
    "    exclude_update_cols = exclude_update_cols or []\n",
    "    update_cols = [c for c in cols if c not in merge_keys + tuple(exclude_update_cols)]\n",
    "    if not update_cols:\n",
    "        raise ValueError(\"No columns to update in MERGE\")\n",
    "\n",
    "    on_clause = \" AND \".join([f\"t.`{k}` = s.`{k}`\" for k in merge_keys])\n",
    "    update_set = \", \".join([f\"t.`{c}` = s.`{c}`\" for c in update_cols])\n",
    "    insert_cols = \", \".join([f\"`{c}`\" for c in cols])\n",
    "    insert_vals = \", \".join([f\"s.`{c}`\" for c in cols])\n",
    "    \n",
    "    # print(\"Before merge\")\n",
    "    # spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "    # [CONFIG_NOT_AVAILABLE] Configuration spark.databricks.delta.schema.autoMerge.enabled is not available. See https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-configuration.html for details.\n",
    "\n",
    "    # Work around for handling schema changes\\\n",
    "    staging_sdf.limit(0).write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(target_table)\n",
    "\n",
    "    merge_sql = f\"\"\"\n",
    "    MERGE INTO {target_table} t\n",
    "    USING {view} s\n",
    "    ON {on_clause}\n",
    "    WHEN MATCHED THEN UPDATE SET {update_set}\n",
    "    WHEN NOT MATCHED THEN INSERT ({insert_cols}) VALUES ({insert_vals})\n",
    "    \"\"\"\n",
    "    print(f\"Executing MERGE into {target_table} ...\")\n",
    "    spark.sql(merge_sql)\n",
    "    recs = staging_sdf.count()\n",
    "\n",
    "    # compute data_timestamp and load_timestamp bounds\n",
    "    min_data_ts = None\n",
    "    max_data_ts = None\n",
    "    max_load_ts = None\n",
    "    if \"data_timestamp\" in cols:\n",
    "        agg = staging_sdf.agg(F.min(\"data_timestamp\").alias(\"min_dt\"), F.max(\"data_timestamp\").alias(\"max_dt\")).collect()[0]\n",
    "        min_data_ts, max_data_ts = agg[\"min_dt\"], agg[\"max_dt\"]\n",
    "    if \"transform_ts\" in cols:\n",
    "        agg2 = staging_sdf.agg(F.max(\"transform_ts\").alias(\"max_lt\")).collect()[0]\n",
    "        max_load_ts = agg2[\"max_lt\"]\n",
    "    # if \"load_timestamp\" in cols:\n",
    "    #     agg2 = staging_sdf.agg(F.max(\"load_timestamp\").alias(\"max_lt\")).collect()[0]\n",
    "    #     max_load_ts = agg2[\"max_lt\"]\n",
    "    # elif \"max_load_timestamp\" in cols:\n",
    "    #     agg2 = staging_sdf.agg(F.max(\"max_load_timestamp\").alias(\"max_lt\")).collect()[0]\n",
    "    #     max_load_ts = agg2[\"max_lt\"]\n",
    "    \n",
    "    print(f\"MERGE completed for {target_table}: {recs} rows, data_ts range=({min_data_ts},{max_data_ts}), max_load_ts={max_load_ts}\")\n",
    "    return recs, min_data_ts, max_data_ts, max_load_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1934c074-3128-4f2b-942d-5c2b74c3ad4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDF24️ Step 7: Create Gold layer Weather Daily Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8b61e96-a71c-43fd-9bb8-6ffad7899cee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_processed = 0 \n",
    "agg_min_data_ts = None\n",
    "agg_max_data_ts = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ef330fc-4669-4c34-88db-850aa13b5767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather Daily: last processed transform_ts = None\nExecuting MERGE into env_catalog.env_data.gold_weather_daily ...\nMERGE completed for env_catalog.env_data.gold_weather_daily: 24 rows, data_ts range=(2025-01-01 00:00:00,2025-11-19 00:00:00), max_load_ts=2025-11-13 11:03:41.374148\nUpdated last processed timestamp for env_catalog.env_data.silver_weather_daily: 2025-11-13 11:03:41.374148\n\uD83C\uDF24️ Daily weather rolling aggregates updated.\n"
     ]
    }
   ],
   "source": [
    "# Build Gold Daily aggregates (rolling Climate index)\n",
    "try:\n",
    "    src = silver_weather_daily_tbl\n",
    "    if not table_exists(src):\n",
    "        print(\"No daily weather table found in Silver layer; skipping daily aggregation.\")\n",
    "    else:\n",
    "        last_load = read_last_processed_load_ts(src)\n",
    "        print(f\"Weather Daily: last processed transform_ts = {last_load}\")\n",
    "\n",
    "        # ensure climate_index exists and is DOUBLE\n",
    "        # Retain only required columns - dim_key, city, data_timestamp, max_load_timestamp, transform_ts\n",
    "        wdf = (\n",
    "            spark.table(src)\n",
    "                .select(\n",
    "                    F.col(\"dim_key\"),\n",
    "                    F.col(\"city\"),\n",
    "                    F.col(\"data_timestamp\"),\n",
    "                    F.col(\"transform_ts\"),\n",
    "                    F.col(\"load_timestamp\"),\n",
    "                    F.col(\"climate_index\").cast(\"double\").alias(\"climate_index\")\n",
    "                )\n",
    "        )\n",
    "        # Drop rows with no climate_index\n",
    "        wdf = wdf.filter(F.col(\"climate_index\").isNotNull())\n",
    "\n",
    "        if last_load:\n",
    "            wdf = wdf.filter(F.col(\"transform_ts\") > F.lit(last_load))\n",
    "        \n",
    "        # Retain only required columns - dim_key, city, data_timestamp, load_timestamp, transform_ts\n",
    "        wdf_rolling = wdf.withColumn(\"data_timestamp\", F.to_timestamp(\"data_timestamp\")) \\\n",
    "            .withColumn(\"load_timestamp\", F.to_timestamp(\"load_timestamp\")) \\\n",
    "            .withColumn(\"transform_ts\", F.to_timestamp(\"transform_ts\"))\n",
    "        # Add required derived columns\n",
    "        wdf_rolling = wdf_rolling.withColumn(\"run_id\", F.lit(run_id))\n",
    "        \n",
    "        # Add rolling values of aqi mean and max values for future 7 days, past 7 days, 30 days ans 90 days\n",
    "        wdf_rolling = get_rolling_agg(wdf_rolling, metric_col=\"climate_index\")\n",
    "        wdf_rolling = climate_category_and_color(wdf_rolling)\n",
    "        \n",
    "        #print(\"Before merge function call\")\n",
    "        recs, min_dt, max_dt, max_lt = merge_into_gold(gold_weather_daily_tbl, wdf_rolling\n",
    "                                                         , merge_keys=(\"city\", \"data_timestamp\"))\n",
    "        \n",
    "        total_processed += recs\n",
    "        if min_dt:\n",
    "            agg_min_data_ts = min_dt if agg_min_data_ts is None else min(agg_min_data_ts, min_dt)\n",
    "        if max_dt:\n",
    "            agg_max_data_ts = max_dt if agg_max_data_ts is None else max(agg_max_data_ts, max_dt)\n",
    "\n",
    "        # commit meta only after successful merge\n",
    "        if max_lt:\n",
    "            commit_last_processed_load_ts(src, max_lt)\n",
    "        print(\"\uD83C\uDF24️ Daily weather rolling aggregates updated.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR while building {gold_weather_daily_tbl}: {str(e)}\")\n",
    "    update_pipeline_log(\"FAILED\", f\"Weather daily rolling aggregation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9087707c-03f2-4d77-b5a3-0d48f08860b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDF24️ Step 8: Create Gold layer Air Daily Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9751dece-dc59-4e8c-8e88-aca162ba36ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Air Daily: last processed transform_ts = None\nStep1\nStep2\nBefore merge function call\nExecuting MERGE into env_catalog.env_data.gold_air_daily ...\nMERGE completed for env_catalog.env_data.gold_air_daily: 22 rows, data_ts range=(2025-01-01 00:00:00,2025-11-18 00:00:00), max_load_ts=2025-11-13 11:03:54.377870\nAfter merge function call\nUpdated last processed timestamp for env_catalog.env_data.silver_air_daily: 2025-11-13 11:03:54.377870\n\uD83C\uDF24️ Daily air rolling aggregates updated.\n"
     ]
    }
   ],
   "source": [
    "# Build Gold Daily aggregates (rolling AQI)\n",
    "try:\n",
    "    src = silver_air_daily_tbl\n",
    "    if not table_exists(src):\n",
    "        print(\"No daily air table found in Silver layer; skipping daily aggregation.\")\n",
    "    else:\n",
    "        last_load = read_last_processed_load_ts(src)\n",
    "        print(f\"Air Daily: last processed transform_ts = {last_load}\")\n",
    "\n",
    "        air = spark.table(src)\n",
    "        # ensure aqi_value exists and is DOUBLE\n",
    "        # Retain only required columns - dim_key, city, data_timestamp, max_load_timestamp, transform_ts\n",
    "        air = (\n",
    "            spark.table(src)\n",
    "                .select(\n",
    "                    F.col(\"dim_key\"),\n",
    "                    F.col(\"city\"),\n",
    "                    F.col(\"data_timestamp\"),\n",
    "                    F.col(\"transform_ts\"),\n",
    "                    F.col(\"max_load_timestamp\").alias(\"load_timestamp\"),\n",
    "                    F.col(\"aqi_value\").cast(\"double\").alias(\"aqi_value\")\n",
    "                )\n",
    "        )\n",
    "        # Drop rows with no aqi_value\n",
    "        air = air.filter(F.col(\"aqi_value\").isNotNull())\n",
    "\n",
    "        if last_load:\n",
    "            air = air.filter(F.col(\"transform_ts\") > F.lit(last_load))\n",
    "        \n",
    "        # Retain only required columns - dim_key, city, data_timestamp, load_timestamp, transform_ts\n",
    "        air_rolling = air.withColumn(\"data_timestamp\", F.to_timestamp(\"data_timestamp\")) \\\n",
    "            .withColumn(\"load_timestamp\", F.to_timestamp(\"load_timestamp\")) \\\n",
    "            .withColumn(\"transform_ts\", F.to_timestamp(\"transform_ts\"))\n",
    "        # Add required derived columns\n",
    "        air_rolling = air_rolling.withColumn(\"run_id\", F.lit(run_id))\n",
    "        print(\"Step1\")\n",
    "        \n",
    "        # Add rolling values of aqi mean and max values for future 7 days, past 7 days, 30 days ans 90 days\n",
    "        air_rolling = get_rolling_agg(air_rolling, metric_col=\"aqi_value\")\n",
    "        print(\"Step2\")\n",
    "        \n",
    "        air_rolling = aqi_category_and_color(air_rolling)\n",
    "        print(\"Before merge function call\")\n",
    "        recs, min_dt, max_dt, max_lt = merge_into_gold(gold_air_daily_tbl, air_rolling\n",
    "                                                         , merge_keys=(\"city\", \"data_timestamp\"))\n",
    "        print(\"After merge function call\")\n",
    "        total_processed += recs\n",
    "        if min_dt:\n",
    "            agg_min_data_ts = min_dt if agg_min_data_ts is None else min(agg_min_data_ts, min_dt)\n",
    "        if max_dt:\n",
    "            agg_max_data_ts = max_dt if agg_max_data_ts is None else max(agg_max_data_ts, max_dt)\n",
    "\n",
    "        # commit meta only after successful merge\n",
    "        if max_lt:\n",
    "            commit_last_processed_load_ts(src, max_lt)\n",
    "        print(\"\uD83C\uDF24️ Daily air rolling aggregates updated.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR while building {gold_air_daily_tbl}: {str(e)}\")\n",
    "    update_pipeline_log(\"FAILED\", f\"Air daily rolling aggregation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c2bf35-445f-4fcf-8240-80cde8fc3137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ✅ Step 9: Finalize and Log Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c7bcb70-04fb-4e5c-a0bf-e4de9cab93d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ pipeline_log updated: SUCCESS\n✅ Gold layer completed successfully. Rows processed: 46\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    update_pipeline_log(\"SUCCESS\", \"Gold layer completed successfully\", total_processed, agg_min_data_ts, agg_max_data_ts)\n",
    "    print(f\"✅ Gold layer completed successfully. Rows processed: {total_processed}\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Final log update failed:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Aggregation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}