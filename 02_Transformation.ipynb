{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8b8417b-4eae-4569-8eb3-63acf557aef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83E\uDE99 Silver Layer: \n",
    "## Data Cleansing, Enrichment, and Aggregation\n",
    "\n",
    "**Purpose:**  \n",
    "This notebook reads data from **Bronze** tables, performs:\n",
    "- Data cleansing  \n",
    "- WHO-style AQI enrichment  \n",
    "- Idempotent upserts into **Silver** tables  \n",
    "- Builds **daily aggregated Air Quality** Silver table  \n",
    "\n",
    "It also updates the `pipeline_log` table for every execution — marking both **success** and **failure**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a146f99e-7b0b-4333-bed6-3d7684ebe6f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83E\uDDF0 Step 1: Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3c5fa77-c447-4840-948a-5e5461a149b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c35aea91-b503-4fda-be20-8ea3dfaf05fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ⚙️ Step 2: Configuration — Table Names and Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c706429-81a2-4193-af95-b571fe6375cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"env_catalog\"\n",
    "schema = \"env_data\"\n",
    "\n",
    "city_master_tbl = f\"{catalog}.{schema}.city_master\"\n",
    "pipeline_log_tbl = f\"{catalog}.{schema}.pipeline_log\"\n",
    "silver_meta_tbl = f\"{catalog}.{schema}.source_last_processed_ts\"\n",
    "\n",
    "bronze_weather_hourly_tbl   = f\"{catalog}.{schema}.bronze_weather_hourly\"\n",
    "bronze_weather_daily_tbl    = f\"{catalog}.{schema}.bronze_weather_daily\"\n",
    "bronze_air_hourly_tbl       = f\"{catalog}.{schema}.bronze_air_hourly\"\n",
    "\n",
    "silver_weather_hourly_tbl = f\"{catalog}.{schema}.silver_weather_hourly\"\n",
    "silver_weather_daily_tbl = f\"{catalog}.{schema}.silver_weather_daily\"\n",
    "silver_air_hourly_tbl     = f\"{catalog}.{schema}.silver_air_hourly\"\n",
    "silver_air_daily_tbl      = f\"{catalog}.{schema}.silver_air_daily\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f2fb5da-e4cd-4783-9528-3631274d0614",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83E\uDEB6 Created pipeline_log entry for run_id=HOURLY_TRANSFORM_20251113_061943\n"
     ]
    }
   ],
   "source": [
    "pipeline_name = \"TRANSFORM\"\n",
    "# TODO: Determine if Manual run or Scheduled run\n",
    "triggered_by = \"Manual\"  # Can be Scheduled/Event driven if automated\n",
    "run_type = \"HOURLY\"\n",
    "start_ts = datetime.now(timezone.utc)\n",
    "run_id = globals().get(\"run_id\", f\"{run_type}_{pipeline_name.upper()}_{start_ts.strftime('%Y%m%d_%H%M%S')}\")\n",
    "status = \"RUNNING\"\n",
    "remarks = \"Silver Transformation layer job started\"\n",
    "\n",
    "# Initial pipeline log entry\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {pipeline_log_tbl} \n",
    "        (run_id, pipeline_name, run_type, start_time, status, triggered_by, remarks, created_ts)\n",
    "        VALUES ('{run_id}', '{pipeline_name}', '{run_type}', TIMESTAMP '{start_ts}', '{status}', '{triggered_by}', '{remarks}', current_timestamp())\n",
    "    \"\"\")\n",
    "    print(f\"\uD83E\uDEB6 Created pipeline_log entry for run_id={run_id}\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Could not log start in pipeline_log:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cdd7920-4284-426f-9e06-fc791d8cd358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83E\uDDE9 Step 3: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b269f6a-c419-427f-8f94-de089b11af08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def table_exists(name: str) -> bool:\n",
    "    \"\"\"Robust check for table existence (works across environments).\"\"\"\n",
    "    try:\n",
    "        # prefer catalog metadata check\n",
    "        if spark.catalog.tableExists(name):\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    # fallback: try to read metadata plan (no data read)\n",
    "    try:\n",
    "        spark.table(name).limit(0).count()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "    \n",
    "def ensure_table(name: str, schema_sdf, partition_col=None):\n",
    "    \"\"\"\n",
    "    Create a Delta table with the provided schema if it does not exist.\n",
    "    Uses schema_sdf.limit(0) to create structure only.\n",
    "    \"\"\"\n",
    "    if table_exists(name):\n",
    "        return\n",
    "    write_builder = schema_sdf.limit(0).write.format(\"delta\") .mode(\"overwrite\").option(\"overwriteSchema\", \"true\")\n",
    "    if partition_col:\n",
    "        write_builder = write_builder.partitionBy(partition_col)\n",
    "    write_builder.saveAsTable(name)\n",
    "    print(f\"Created table {name}\")\n",
    "\n",
    "def update_pipeline_log(status, remarks, records_processed=0, earliest_ts=None, latest_ts=None):\n",
    "    \"\"\"Update the pipeline_log for this run.\"\"\"\n",
    "    try:\n",
    "        earliest_expr = f\"TIMESTAMP '{earliest_ts}'\" if earliest_ts else \"NULL\"\n",
    "        latest_expr = f\"TIMESTAMP '{latest_ts}'\" if latest_ts else \"NULL\"\n",
    "        spark.sql(f\"\"\"\n",
    "            UPDATE {pipeline_log_tbl}\n",
    "            SET end_time = current_timestamp(),\n",
    "                status = '{status}',\n",
    "                records_processed = COALESCE(records_processed, 0) + {records_processed},\n",
    "                earliest_ts = COALESCE(earliest_ts, {earliest_expr}),\n",
    "                latest_ts = {latest_expr},\n",
    "                remarks = '{remarks}'\n",
    "            WHERE run_id = '{run_id}'\n",
    "        \"\"\")\n",
    "        print(f\"✅ pipeline_log updated: {status}\")\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Could not update pipeline_log:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a1ef4cb-c4be-41c4-8aad-01af0c266e05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83E\uDDFC Step 4: Data Cleansing and Data Enrichment Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "604f2270-9c2a-4bab-85c9-55412f5ed1b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def basic_cleanse(df, key_cols):\n",
    "    \"\"\"Remove nulls, deduplicate, normalize timestamps.\"\"\"\n",
    "    for k in key_cols:\n",
    "        df = df.filter(F.col(k).isNotNull())\n",
    "    if \"timestamp\" in df.columns:\n",
    "        df = df.withColumn(\"timestamp\", F.to_timestamp(F.col(\"timestamp\")))\n",
    "    df = df.dropDuplicates(key_cols)\n",
    "    return df\n",
    "\n",
    "def derive_weather_description(sdf):\n",
    "    # Ref: https://open-meteo.com/en/docs#daily_weather_variables\n",
    "    # WMO Weather interpretation codes (WW)\n",
    "    # Code\tDescription\n",
    "    # 0\tClear sky\n",
    "    # 1, 2, 3\tMainly clear, partly cloudy, and overcast\n",
    "    # 45, 48\tFog and depositing rime fog\n",
    "    # 51, 53, 55\tDrizzle: Light, moderate, and dense intensity\n",
    "    # 56, 57\tFreezing Drizzle: Light and dense intensity\n",
    "    # 61, 63, 65\tRain: Slight, moderate and heavy intensity\n",
    "    # 66, 67\tFreezing Rain: Light and heavy intensity\n",
    "    # 71, 73, 75\tSnow fall: Slight, moderate, and heavy intensity\n",
    "    # 77\tSnow grains\n",
    "    # 80, 81, 82\tRain showers: Slight, moderate, and violent\n",
    "    # 85, 86\tSnow showers slight and heavy\n",
    "    # 95 *\tThunderstorm: Slight or moderate\n",
    "    # 96, 99 *\tThunderstorm with slight and heavy hail\n",
    "    wc_col = \"weather_code\"\n",
    "\n",
    "    wc = F.col(wc_col).cast(\"int\")\n",
    "\n",
    "    # Compact labels based on Open-Meteo / WMO weather codes\n",
    "    sdf = sdf.withColumn(\n",
    "        \"weather_description\",\n",
    "        F.when(wc == 0, \"Clear sky\")\n",
    "         .when((wc >= 1) & (wc <= 3), \"Mainly clear / partly cloudy / overcast\")\n",
    "         .when((wc >= 45) & (wc <= 48), \"Fog / depositing rime fog\")\n",
    "         .when((wc >= 51) & (wc <= 55), \"Drizzle (light / moderate / dense)\")\n",
    "         .when((wc >= 56) & (wc <= 57), \"Freezing drizzle\")\n",
    "         .when((wc >= 61) & (wc <= 65), \"Rain (light / moderate / heavy)\")\n",
    "         .when((wc >= 66) & (wc <= 67), \"Freezing rain\")\n",
    "         .when((wc >= 71) & (wc <= 75), \"Snow fall (light / moderate / heavy)\")\n",
    "         .when(wc == 77, \"Snow grains\")\n",
    "         .when((wc >= 80) & (wc <= 82), \"Rain showers (slight / moderate / violent)\")\n",
    "         .when((wc >= 85) & (wc <= 86), \"Snow showers (slight / moderate)\")\n",
    "         .when((wc == 95), \"Thunderstorm (slight / moderate rain)\")\n",
    "         .when((wc >= 96) & (wc <= 99), \"Thunderstorm with hail\")\n",
    "         .otherwise(\"Unknown / Other\")\n",
    "    )\n",
    "    return sdf\n",
    "\n",
    "# Compute single climate_index (0-100) + climate_label \n",
    "# It will run only for weather tables (hourly/daily) and is safe if columns are missing.\n",
    "def compute_climate_index_and_label(sdf):\n",
    "    \"\"\"\n",
    "    Adds two columns:\n",
    "      - climate_index (double) in [0,100]\n",
    "      - climate_label (string) one of: Poor, Fair, Moderate, Good, Excellent\n",
    "    Uses existing columns if present: temperature_c, relative_humidity, pressure_hpa,\n",
    "    precipitation_mm (hourly) or precipitation_sum (daily), snow_mm, wind_speed_m_s.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure expected columns exist (create null if missing)\n",
    "    def _ensure(colname):\n",
    "        return colname if colname in sdf.columns else F.lit(None).cast(\"double\").alias(colname)\n",
    "\n",
    "    # Prefer precipitation_sum for daily; fallback to precipitation_mm\n",
    "    temp_col = \"temperature_2m\" if \"temperature_2m\" in sdf.columns else \"temperature_2m_mean\"\n",
    "    humd_col = \"relative_humidity_2m\" if \"relative_humidity_2m\" in sdf.columns else \"relative_humidity_2m_mean\"\n",
    "    phpa_col = \"pressure_msl\" if \"pressure_msl\" in sdf.columns else \"pressure_msl_mean\"\n",
    "    prec_col = \"precipitation\" if \"precipitation\" in sdf.columns else \"precipitation_sum\"\n",
    "    snow_col = \"snowfall\" if \"snowfall\" in sdf.columns else \"snowfall_sum\"\n",
    "    wsms_col = \"wind_speed_10m\" if \"wind_speed_10m\" in sdf.columns else \"wind_speed_10m_max\"  \n",
    "\n",
    "    # create safe column refs (these are Column expressions)\n",
    "    t_col   = sdf[temp_col] if temp_col in sdf.columns else F.lit(None).cast(\"double\")\n",
    "    rh_col  = sdf[humd_col] if humd_col in sdf.columns else F.lit(None).cast(\"double\")\n",
    "    p_col   = sdf[phpa_col] if phpa_col in sdf.columns else F.lit(None).cast(\"double\")\n",
    "    pr_col  = sdf[prec_col] if prec_col in sdf.columns else F.lit(None).cast(\"double\")\n",
    "    sno_col = sdf[snow_col] if snow_col in sdf.columns else F.lit(0.0).cast(\"double\")\n",
    "    ws_col  = sdf[wsms_col] if wsms_col in sdf.columns else F.lit(None).cast(\"double\")\n",
    "\n",
    "    # Subscore calculations (each in 0..100)\n",
    "    # Temperature: ideal band 18..26°C -> 100, linear decay outside; clamp to [0,100]\n",
    "    temp_sub = (F.when(t_col.between(18.0,26.0), F.lit(100.0))\n",
    "                 .when(t_col.isNull(), F.lit(None))\n",
    "                 .otherwise(\n",
    "                    # linear decay: score = 100 * max(0, 1 - (span/40))\n",
    "                    F.least(F.lit(100.0),\n",
    "                            F.greatest(F.lit(0.0),\n",
    "                                100.0 * (1 - (F.when(t_col < 18.0, (18.0 - t_col)).otherwise(t_col - 26.0) / F.lit(40.0)))\n",
    "                            )\n",
    "                    )\n",
    "                 )\n",
    "               )\n",
    "\n",
    "    # Humidity: ideal 30..60% -> 100, linear decay to 0 over span 60\n",
    "    hum_sub = (F.when(rh_col.between(30.0,60.0), F.lit(100.0))\n",
    "               .when(rh_col.isNull(), F.lit(None))\n",
    "               .otherwise(\n",
    "                   F.least(F.lit(100.0),\n",
    "                           F.greatest(F.lit(0.0),\n",
    "                               100.0 * (1 - (F.when(rh_col < 30.0, (30.0 - rh_col)).otherwise(rh_col - 60.0) / F.lit(60.0))))\n",
    "                          )\n",
    "               )\n",
    "              )\n",
    "\n",
    "    # Pressure: nominal 1013.25 hPa within 20 -> 100, linear decay to 0 at 3*20\n",
    "    pres_sub = (F.when(p_col.isNull(), F.lit(None))\n",
    "                .otherwise(\n",
    "                    F.when(F.abs(p_col - F.lit(1013.25)) <= F.lit(20.0), F.lit(100.0))\n",
    "                     .otherwise(\n",
    "                        F.least(F.lit(100.0),\n",
    "                                F.greatest(F.lit(0.0),\n",
    "                                    100.0 * (1 - ((F.abs(p_col - F.lit(1013.25)) - F.lit(20.0)) / (F.lit(60.0))))\n",
    "                                )\n",
    "                        )\n",
    "                     )\n",
    "                )\n",
    "               )\n",
    "\n",
    "    # Precipitation: 0 mm -> 100, penalty per mm => clamp at 0\n",
    "    precip_total = F.coalesce(pr_col, F.lit(0.0)) #+ F.coalesce(sno_col, F.lit(0.0))\n",
    "    precip_sub = F.when(pr_col.isNull() , F.lit(None)) \\\n",
    "                  .otherwise(F.least(F.lit(100.0), F.greatest(F.lit(0.0), F.lit(100.0) - precip_total * F.lit(1.5))))\n",
    "\n",
    "    # Wind: <=2 m/s -> 100, mild penalty up to 5 m/s, strong penalty above\n",
    "    wind_sub = (F.when(ws_col.isNull(), F.lit(None))\n",
    "                .when(ws_col <= F.lit(2.0), F.lit(100.0))\n",
    "                .when(ws_col <= F.lit(5.0), F.lit(100.0) * (1 - ((ws_col - F.lit(2.0)) / F.lit(3.0)) * F.lit(0.5)))\n",
    "                .otherwise(F.least(F.lit(100.0), F.greatest(F.lit(0.0), F.lit(100.0) * (1 - ((ws_col - F.lit(5.0)) / F.lit(20.0))))))\n",
    "               )\n",
    "\n",
    "    # Combine subscores with weights (temperature 35%, humidity 25%, precip 20%, pressure 10%, wind 10%)\n",
    "    # If some subscores are NULL, re-normalize weights among present subscores\n",
    "    # Build weighted_sum = sum(sub_i * weight_i) / sum(weights_present)\n",
    "    w_temp, w_hum, w_precip, w_pres, w_wind = 0.35, 0.25, 0.20, 0.10, 0.10\n",
    "\n",
    "    # compute numerator and denominator safely\n",
    "    numer = (F.when(temp_sub.isNotNull(), temp_sub * F.lit(w_temp)).otherwise(F.lit(0.0))\n",
    "             + F.when(hum_sub.isNotNull(), hum_sub * F.lit(w_hum)).otherwise(F.lit(0.0))\n",
    "             + F.when(precip_sub.isNotNull(), precip_sub * F.lit(w_precip)).otherwise(F.lit(0.0))\n",
    "             + F.when(pres_sub.isNotNull(), pres_sub * F.lit(w_pres)).otherwise(F.lit(0.0))\n",
    "             + F.when(wind_sub.isNotNull(), wind_sub * F.lit(w_wind)).otherwise(F.lit(0.0))\n",
    "            )\n",
    "\n",
    "    denom = (F.when(temp_sub.isNotNull(), F.lit(w_temp)).otherwise(F.lit(0.0))\n",
    "             + F.when(hum_sub.isNotNull(), F.lit(w_hum)).otherwise(F.lit(0.0))\n",
    "             + F.when(precip_sub.isNotNull(), F.lit(w_precip)).otherwise(F.lit(0.0))\n",
    "             + F.when(pres_sub.isNotNull(), F.lit(w_pres)).otherwise(F.lit(0.0))\n",
    "             + F.when(wind_sub.isNotNull(), F.lit(w_wind)).otherwise(F.lit(0.0))\n",
    "            )\n",
    "\n",
    "    climate_index = F.when(denom == 0, F.lit(None)).otherwise(F.round(numer / denom, 2)).alias(\"climate_index\")\n",
    "\n",
    "    # label mapping\n",
    "    climate_label = F.when(climate_index.isNull(), F.lit(None)) \\\n",
    "                     .when(climate_index <= F.lit(20.0), F.lit(\"Poor\")) \\\n",
    "                     .when(climate_index <= F.lit(40.0), F.lit(\"Fair\")) \\\n",
    "                     .when(climate_index <= F.lit(60.0), F.lit(\"Moderate\")) \\\n",
    "                     .when(climate_index <= F.lit(80.0), F.lit(\"Good\")) \\\n",
    "                     .otherwise(F.lit(\"Excellent\")).alias(\"climate_label\")\n",
    "\n",
    "    # attach intermediate subs (optional: include only those which need to be stored)\n",
    "    sdf_with = (sdf\n",
    "                # .withColumn(\"__temp_sub\", temp_sub)\n",
    "                # .withColumn(\"__hum_sub\", hum_sub)\n",
    "                # .withColumn(\"__pres_sub\", pres_sub)\n",
    "                # .withColumn(\"__precip_sub\", precip_sub)\n",
    "                # .withColumn(\"__wind_sub\", wind_sub)\n",
    "                .withColumn(\"climate_index\", climate_index)\n",
    "                .withColumn(\"climate_label\", climate_label)\n",
    "               )\n",
    "\n",
    "    return sdf_with\n",
    "\n",
    "# Compute AQI (0–500) + category + color for hourly & daily -----\n",
    "# It will run only for air pollution tables (hourly/daily) and is safe if columns are missing.\n",
    "def compute_aqi_index_and_label(sdf):\n",
    "    \"\"\"\n",
    "    Adds:\n",
    "      - aqi_value (int, 0–500)\n",
    "      - aqi_category (string)\n",
    "      - aqi_color (string)\n",
    "    Automatically detects hourly vs daily based on column suffix (_mean)\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine whether to use base or _mean columns\n",
    "    use_suffix = \"_mean\" if any(c.endswith(\"_mean\") for c in sdf.columns) else \"\"\n",
    "\n",
    "    # Construct pollutant column names dynamically\n",
    "    pollutants = {\n",
    "        \"pm25\": f\"pm2_5{use_suffix}\",\n",
    "        \"pm10\": f\"pm10{use_suffix}\",\n",
    "        \"o3\": f\"ozone{use_suffix}\",\n",
    "        \"no2\": f\"nitrogen_dioxide{use_suffix}\",\n",
    "        \"so2\": f\"sulphur_dioxide{use_suffix}\",\n",
    "        \"co\": f\"carbon_monoxide{use_suffix}\"\n",
    "    }\n",
    "\n",
    "    # Ensure all columns exist\n",
    "    for _, col_name in pollutants.items():\n",
    "        if col_name not in sdf.columns:\n",
    "            sdf = sdf.withColumn(col_name, F.lit(None).cast(\"double\"))\n",
    "\n",
    "    # Breakpoint tables (EPA/AirNow)\n",
    "    PM25 = [(0,12,0,50),(12.1,35.4,51,100),(35.5,55.4,101,150),(55.5,150.4,151,200),\n",
    "            (150.5,250.4,201,300),(250.5,350.4,301,400),(350.5,500.4,401,500)]\n",
    "    PM10 = [(0,54,0,50),(55,154,51,100),(155,254,101,150),(255,354,151,200),\n",
    "            (355,424,201,300),(425,504,301,400),(505,604,401,500)]\n",
    "    O3   = [(0,54,0,50),(55,70,51,100),(71,85,101,150),(86,105,151,200),(106,200,201,300)]\n",
    "    NO2  = [(0,53,0,50),(54,100,51,100),(101,360,101,150),(361,649,151,200),\n",
    "            (650,1249,201,300),(1250,1649,301,400),(1650,2049,401,500)]\n",
    "    SO2  = [(0,35,0,50),(36,75,51,100),(76,185,101,150),(186,304,151,200),\n",
    "            (305,604,201,300),(605,804,301,400),(805,1004,401,500)]\n",
    "    CO   = [(0,4499,0,50),(4500,9499,51,100),(9500,12499,101,150),(12500,15499,151,200),\n",
    "            (15500,30499,201,300),(30500,40499,301,400),(40500,50499,401,500)]\n",
    "\n",
    "    def _build_aqi_expr(col, breaks):\n",
    "        expr = F.lit(None)\n",
    "        for (Clow, Chigh, Ilow, Ihigh) in breaks:\n",
    "            slope = (Ihigh - Ilow) / (Chigh - Clow)\n",
    "            interp = (slope * (F.col(col) - F.lit(Clow)) + F.lit(Ilow))\n",
    "            expr = F.when(F.col(col).between(Clow, Chigh), F.round(interp))\\\n",
    "                     .otherwise(expr)\n",
    "        expr = F.when(F.col(col).isNull(), F.lit(None)).otherwise(\n",
    "            F.when(F.col(col) > breaks[-1][1], F.lit(500)).otherwise(expr)\n",
    "        )\n",
    "        return expr\n",
    "\n",
    "    # Compute pollutant-specific AQI\n",
    "    sdf = (sdf\n",
    "        .withColumn(\"aqi_pm25\", _build_aqi_expr(pollutants[\"pm25\"], PM25))\n",
    "        .withColumn(\"aqi_pm10\", _build_aqi_expr(pollutants[\"pm10\"], PM10))\n",
    "        .withColumn(\"aqi_o3\",   _build_aqi_expr(pollutants[\"o3\"], O3))\n",
    "        .withColumn(\"aqi_no2\",  _build_aqi_expr(pollutants[\"no2\"], NO2))\n",
    "        .withColumn(\"aqi_so2\",  _build_aqi_expr(pollutants[\"so2\"], SO2))\n",
    "        .withColumn(\"aqi_co\",   _build_aqi_expr(pollutants[\"co\"], CO))\n",
    "    )\n",
    "\n",
    "    # Final AQI = max across pollutants\n",
    "    sdf = sdf.withColumn(\n",
    "        \"aqi_value\",\n",
    "        F.when(\n",
    "            F.greatest(\n",
    "                F.coalesce(F.col(\"aqi_pm25\"), F.lit(-1)),\n",
    "                F.coalesce(F.col(\"aqi_pm10\"), F.lit(-1)),\n",
    "                F.coalesce(F.col(\"aqi_o3\"), F.lit(-1)),\n",
    "                F.coalesce(F.col(\"aqi_no2\"), F.lit(-1)),\n",
    "                F.coalesce(F.col(\"aqi_so2\"), F.lit(-1)),\n",
    "                F.coalesce(F.col(\"aqi_co\"), F.lit(-1))\n",
    "            ) < 0, F.lit(None)\n",
    "        ).otherwise(\n",
    "            F.greatest(\n",
    "                F.coalesce(F.col(\"aqi_pm25\"), F.lit(-1)),\n",
    "                F.coalesce(F.col(\"aqi_pm10\"), F.lit(-1)),\n",
    "                F.coalesce(F.col(\"aqi_o3\"), F.lit(-1)),\n",
    "                F.coalesce(F.col(\"aqi_no2\"), F.lit(-1)),\n",
    "                F.coalesce(F.col(\"aqi_so2\"), F.lit(-1)),\n",
    "                F.coalesce(F.col(\"aqi_co\"), F.lit(-1))\n",
    "            ).cast(\"int\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Primary pollutant\n",
    "    sdf = sdf.withColumn(\"aqi_primary_pollutant\",\n",
    "        F.when(F.col(\"aqi_value\") < 0, F.lit(None))\n",
    "        .when(F.col(\"aqi_value\") == F.coalesce(F.col(\"aqi_pm25\"), F.lit(-1)), F.lit(\"pm25\"))\n",
    "        .when(F.col(\"aqi_value\") == F.coalesce(F.col(\"aqi_pm10\"), F.lit(-1)), F.lit(\"pm10\"))\n",
    "        .when(F.col(\"aqi_value\") == F.coalesce(F.col(\"aqi_o3\"), F.lit(-1)), F.lit(\"o3\"))\n",
    "        .when(F.col(\"aqi_value\") == F.coalesce(F.col(\"aqi_no2\"), F.lit(-1)), F.lit(\"no2\"))\n",
    "        .when(F.col(\"aqi_value\") == F.coalesce(F.col(\"aqi_so2\"), F.lit(-1)), F.lit(\"so2\"))\n",
    "        .when(F.col(\"aqi_value\") == F.coalesce(F.col(\"aqi_co\"), F.lit(-1)), F.lit(\"co\"))\n",
    "        .otherwise(F.lit(None))\n",
    "    )\n",
    "\n",
    "    # Category & color mapping\n",
    "    sdf = (sdf\n",
    "        .withColumn(\"aqi_category\",\n",
    "            F.when(F.col(\"aqi_value\").isNull(), F.lit(None))\n",
    "             .when(F.col(\"aqi_value\") <= 50, \"Good\")\n",
    "             .when(F.col(\"aqi_value\") <= 100, \"Moderate\")\n",
    "             .when(F.col(\"aqi_value\") <= 150, \"Unhealthy for Sensitive Groups\")\n",
    "             .when(F.col(\"aqi_value\") <= 200, \"Unhealthy\")\n",
    "             .when(F.col(\"aqi_value\") <= 300, \"Very Unhealthy\")\n",
    "             .otherwise(\"Hazardous\")\n",
    "        )\n",
    "        .withColumn(\"aqi_color\",\n",
    "            F.when(F.col(\"aqi_value\").isNull(), F.lit(None))\n",
    "             .when(F.col(\"aqi_value\") <= 50, \"Green\") #\"#00E400\")\n",
    "             .when(F.col(\"aqi_value\") <= 100, \"Yellow\") #\"#FFFF00\")\n",
    "             .when(F.col(\"aqi_value\") <= 150, \"Orange\") #\"#FF7E00\")\n",
    "             .when(F.col(\"aqi_value\") <= 200, \"Red\") #\"#FF0000\")\n",
    "             .when(F.col(\"aqi_value\") <= 300, \"Purple\") #\"#99004C\")\n",
    "             .otherwise(\"Maroon\") #(\"#7E0023\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Drop temporary pollutant-specific AQIs to keep schema clean\n",
    "    sdf = sdf.drop(\"aqi_pm25\",\"aqi_pm10\",\"aqi_o3\",\"aqi_no2\",\"aqi_so2\",\"aqi_co\")\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c55cb81-c230-4a90-9344-0f718c65be8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ⚡ Step 5: Function to capture last load/transform timestamp from source table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a181afb-7a48-4aac-bd19-8e962e03318e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_last_processed_load_ts(source_table: str):\n",
    "    \"\"\"Return last_processed_load_ts timestamp or None\"\"\"\n",
    "    if not table_exists(silver_meta_tbl):\n",
    "        return None\n",
    "    try:\n",
    "        rows = spark.table(silver_meta_tbl).filter(F.col(\"source_table\") == source_table).select(\"last_processed_load_ts\").limit(1).collect()\n",
    "        return rows[0][\"last_processed_load_ts\"] if rows else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def commit_last_processed_load_ts(source_table: str, last_ts):\n",
    "    \"\"\"Upsert last_processed_load_ts into meta table (run after successful merge)\"\"\"\n",
    "    if last_ts is None:\n",
    "        return\n",
    "    now_ts = datetime.now(timezone.utc)\n",
    "    staging = spark.createDataFrame([(source_table, last_ts, now_ts)], schema=\"source_table STRING, last_processed_load_ts TIMESTAMP, updated_ts TIMESTAMP\")\n",
    "    if not table_exists(silver_meta_tbl):\n",
    "        staging.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_meta_tbl)\n",
    "        return\n",
    "    DeltaTable.forName(spark, silver_meta_tbl).alias(\"t\").merge(\n",
    "        staging.alias(\"s\"),\n",
    "        \"t.source_table = s.source_table\"\n",
    "    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "    print(f\"Updated last processed timestamp for {source_table}: {last_ts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4031aa5d-53e4-4c8e-b58e-fddee5329b4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83D\uDD04 Step 6: Merge Incremental Data into Silver Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30336d04-4e4f-4dc7-a8e5-2c8c72cca8f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MERGE on (city, data_timestamp)\n",
    "def merge_into_silver(target_table: str, staging_sdf, merge_keys=(\"city\", \"data_timestamp\"), exclude_update_cols=None, partition_col=None):\n",
    "    \"\"\"\n",
    "    Idempotent upsert via Delta MERGE\n",
    "    Ensure target exists and MERGE the staging_sdf into it.\n",
    "    Returns rows_processed, min_data_ts, max_data_ts, max_load_ts (if load_timestamp present)\n",
    "    \"\"\"\n",
    "    #print(\"Entered merge\")\n",
    "    if staging_sdf is None:\n",
    "        print(f\"Skipping {target_table} - no staging\")\n",
    "        return 0, None, None, None\n",
    "    \n",
    "    # Coerce data_timestamp/load_timestamp types if present\n",
    "    if \"data_timestamp\" in staging_sdf.columns:\n",
    "        staging_sdf = staging_sdf.withColumn(\"data_timestamp\", F.to_timestamp(\"data_timestamp\"))\n",
    "    if \"load_timestamp\" in staging_sdf.columns:\n",
    "        staging_sdf = staging_sdf.withColumn(\"load_timestamp\", F.to_timestamp(\"load_timestamp\"))\n",
    "\n",
    "    # print(\"Before table creation\")\n",
    "    # ensure target exists\n",
    "    ensure_table(target_table, staging_sdf, partition_col=partition_col)\n",
    "    \n",
    "    # prepare temp view for staging\n",
    "    view = f\"temp_{target_table.split('.')[-1]}_{int(datetime.now().timestamp())}\"\n",
    "    staging_sdf.createOrReplaceTempView(view)\n",
    "\n",
    "    cols = staging_sdf.columns\n",
    "    exclude_update_cols = exclude_update_cols or []\n",
    "    update_cols = [c for c in cols if c not in merge_keys + tuple(exclude_update_cols)]\n",
    "    if not update_cols:\n",
    "        raise ValueError(\"No columns to update in MERGE\")\n",
    "\n",
    "    on_clause = \" AND \".join([f\"t.`{k}` = s.`{k}`\" for k in merge_keys])\n",
    "    update_set = \", \".join([f\"t.`{c}` = s.`{c}`\" for c in update_cols])\n",
    "    insert_cols = \", \".join([f\"`{c}`\" for c in cols])\n",
    "    insert_vals = \", \".join([f\"s.`{c}`\" for c in cols])\n",
    "    \n",
    "    # print(\"Before merge\")\n",
    "    # spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "    # [CONFIG_NOT_AVAILABLE] Configuration spark.databricks.delta.schema.autoMerge.enabled is not available. See https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-configuration.html for details.\n",
    "\n",
    "    # Work around for handling schema changes\\\n",
    "    staging_sdf.limit(0).write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(target_table)\n",
    "\n",
    "    merge_sql = f\"\"\"\n",
    "    MERGE INTO {target_table} t\n",
    "    USING {view} s\n",
    "    ON {on_clause}\n",
    "    WHEN MATCHED THEN UPDATE SET {update_set}\n",
    "    WHEN NOT MATCHED THEN INSERT ({insert_cols}) VALUES ({insert_vals})\n",
    "    \"\"\"\n",
    "    print(f\"Executing MERGE into {target_table} ...\")\n",
    "    spark.sql(merge_sql)\n",
    "    recs = staging_sdf.count()\n",
    "\n",
    "    # compute data_timestamp and load_timestamp bounds\n",
    "    min_data_ts = None\n",
    "    max_data_ts = None\n",
    "    max_load_ts = None\n",
    "    if \"data_timestamp\" in cols:\n",
    "        agg = staging_sdf.agg(F.min(\"data_timestamp\").alias(\"min_dt\"), F.max(\"data_timestamp\").alias(\"max_dt\")).collect()[0]\n",
    "        min_data_ts, max_data_ts = agg[\"min_dt\"], agg[\"max_dt\"]\n",
    "    if \"load_timestamp\" in cols:\n",
    "        agg2 = staging_sdf.agg(F.max(\"load_timestamp\").alias(\"max_lt\")).collect()[0]\n",
    "        max_load_ts = agg2[\"max_lt\"]\n",
    "    elif \"max_load_timestamp\" in cols:\n",
    "        agg2 = staging_sdf.agg(F.max(\"max_load_timestamp\").alias(\"max_lt\")).collect()[0]\n",
    "        max_load_ts = agg2[\"max_lt\"]\n",
    "    \n",
    "    print(f\"MERGE completed for {target_table}: {recs} rows, data_ts range=({min_data_ts},{max_data_ts}), max_load_ts={max_load_ts}\")\n",
    "    return recs, min_data_ts, max_data_ts, max_load_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1934c074-3128-4f2b-942d-5c2b74c3ad4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83E\uDDEE Step 7: Process Weather and Air Hourly Data (Bronze ➜ Silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e520a27-7864-422d-8ebe-13ceb32c4811",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather Hourly: last processed load_timestamp = 2025-11-13 06:03:07.528229\nExecuting MERGE into env_catalog.env_data.silver_weather_hourly ...\nMERGE completed for env_catalog.env_data.silver_weather_hourly: 0 rows, data_ts range=(None,None), max_load_ts=None\nAir Hourly: last processed load_timestamp = 2025-11-13 06:03:07.530387\nExecuting MERGE into env_catalog.env_data.silver_air_hourly ...\nMERGE completed for env_catalog.env_data.silver_air_hourly: 0 rows, data_ts range=(None,None), max_load_ts=None\nWeather Daily: last processed load_timestamp = 2025-11-13 06:03:07.529117\nExecuting MERGE into env_catalog.env_data.silver_weather_daily ...\nMERGE completed for env_catalog.env_data.silver_weather_daily: 0 rows, data_ts range=(None,None), max_load_ts=None\n✅ Total rows merged into Silver: 0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    total_processed = 0\n",
    "    agg_min_data_ts = None\n",
    "    agg_max_data_ts = None\n",
    "\n",
    "    # 1️⃣ Weather Hourly\n",
    "    src = bronze_weather_hourly_tbl\n",
    "    if table_exists(src):\n",
    "        last_load = read_last_processed_load_ts(src)\n",
    "        print(f\"Weather Hourly: last processed load_timestamp = {last_load}\")\n",
    "\n",
    "        wdf = spark.table(src)\n",
    "        if last_load:\n",
    "            wdf = wdf.filter(F.col(\"load_timestamp\") > F.lit(last_load))\n",
    "        #wdf = basic_cleanse(wdf, [\"city\", \"data_timestamp\"])\n",
    "        wdf = derive_weather_description(wdf)\n",
    "        wdf = compute_climate_index_and_label(wdf)\n",
    "        # add transform_ts and Update run_id into staging\n",
    "        wdf = wdf.withColumn(\"transform_ts\", F.current_timestamp()) \\\n",
    "                 .withColumn(\"run_id\", F.lit(run_id))\n",
    "        # Merge into silver\n",
    "        recs, min_dt, max_dt, max_lt = merge_into_silver(silver_weather_hourly_tbl, wdf\n",
    "                                                         , merge_keys=(\"city\", \"data_timestamp\"))\n",
    "        total_processed += recs\n",
    "        if min_dt:\n",
    "            agg_min_data_ts = min_dt if agg_min_data_ts is None else min(agg_min_data_ts, min_dt)\n",
    "        if max_dt:\n",
    "            agg_max_data_ts = max_dt if agg_max_data_ts is None else max(agg_max_data_ts, max_dt)\n",
    "        # commit meta only after successful merge\n",
    "        if max_lt:\n",
    "            commit_last_processed_load_ts(src, max_lt)\n",
    "    else:\n",
    "        print(f\"Source {src} not present - skipping weather hourly\")\n",
    "\n",
    "   # 2️⃣ Air Hourly\n",
    "    src = bronze_air_hourly_tbl\n",
    "    if table_exists(src):\n",
    "        last_load = read_last_processed_load_ts(src)\n",
    "        print(f\"Air Hourly: last processed load_timestamp = {last_load}\")\n",
    "\n",
    "        air_df = spark.table(src)\n",
    "        if last_load:\n",
    "            air_df = air_df.filter(F.col(\"load_timestamp\") > F.lit(last_load))\n",
    "        #air_df = basic_cleanse(air_df, [\"city\", \"data_timestamp\"])\n",
    "        air_df = compute_aqi_index_and_label(air_df)\n",
    "        \n",
    "        # add transform_ts and Update run_id into staging\n",
    "        air_df = air_df.withColumn(\"transform_ts\", F.current_timestamp()) \\\n",
    "                 .withColumn(\"run_id\", F.lit(run_id))\n",
    "        # Merge into silver\n",
    "        recs, min_dt, max_dt, max_lt = merge_into_silver(silver_air_hourly_tbl, air_df\n",
    "                                                         , merge_keys=(\"city\", \"data_timestamp\"))\n",
    "        total_processed += recs\n",
    "        if min_dt:\n",
    "            agg_min_data_ts = min_dt if agg_min_data_ts is None else min(agg_min_data_ts, min_dt)\n",
    "        if max_dt:\n",
    "            agg_max_data_ts = max_dt if agg_max_data_ts is None else max(agg_max_data_ts, max_dt)\n",
    "        # commit meta only after successful merge\n",
    "        if max_lt:\n",
    "            commit_last_processed_load_ts(src, max_lt)\n",
    "    else:\n",
    "        print(f\"Source {src} not present - skipping air hourly\")\n",
    "\n",
    "    # 3️⃣ Weather Daily\n",
    "    src = bronze_weather_daily_tbl\n",
    "    if table_exists(src):\n",
    "        last_load = read_last_processed_load_ts(src)\n",
    "        print(f\"Weather Daily: last processed load_timestamp = {last_load}\")\n",
    "\n",
    "        wd_df = spark.table(src)\n",
    "        if last_load:\n",
    "            wd_df = wd_df.filter(F.col(\"load_timestamp\") > F.lit(last_load))\n",
    "        #wd_df = basic_cleanse(wd_df, [\"city\", \"data_timestamp\"])\n",
    "        wd_df = derive_weather_description(wd_df)\n",
    "        wd_df = compute_climate_index_and_label(wd_df)\n",
    "        \n",
    "        # add transform_ts and Update run_id into staging\n",
    "        wd_df = wd_df.withColumn(\"transform_ts\", F.current_timestamp()) \\\n",
    "                 .withColumn(\"run_id\", F.lit(run_id))\n",
    "        # Merge into silver\n",
    "        recs, min_dt, max_dt, max_lt = merge_into_silver(silver_weather_daily_tbl, wd_df\n",
    "                                                         , merge_keys=(\"city\", \"data_timestamp\"))\n",
    "        total_processed += recs\n",
    "        if min_dt:\n",
    "            agg_min_data_ts = min_dt if agg_min_data_ts is None else min(agg_min_data_ts, min_dt)\n",
    "        if max_dt:\n",
    "            agg_max_data_ts = max_dt if agg_max_data_ts is None else max(agg_max_data_ts, max_dt)\n",
    "        # commit meta only after successful merge\n",
    "        if max_lt:\n",
    "            commit_last_processed_load_ts(src, max_lt)\n",
    "    else:\n",
    "        print(f\"Source {src} not present - skipping weather daily\")\n",
    "    \n",
    "    print(f\"✅ Total rows merged into Silver: {total_processed}\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Silver merge failed:\", e)\n",
    "    update_pipeline_log(\"FAILED\", f\"Silver layer failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9087707c-03f2-4d77-b5a3-0d48f08860b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDF24️ Step 8: Create Silver Air Daily Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9751dece-dc59-4e8c-8e88-aca162ba36ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Air Daily: last processed load_timestamp = 2025-11-13 06:22:16.797663\nExecuting MERGE into env_catalog.env_data.silver_air_daily ...\nMERGE completed for env_catalog.env_data.silver_air_daily: 0 rows, data_ts range=(None,None), max_load_ts=None\n\uD83C\uDF24️ Daily air aggregates updated.\n"
     ]
    }
   ],
   "source": [
    "# Build Silver Air Daily aggregates (derived_AQI mean/max/min)\n",
    "try:\n",
    "    src = silver_air_hourly_tbl\n",
    "    if not table_exists(src):\n",
    "        print(\"No hourly air table found; skipping daily aggregation.\")\n",
    "    else:\n",
    "        last_load = read_last_processed_load_ts(src)\n",
    "        print(f\"Air Daily: last processed load_timestamp = {last_load}\")\n",
    "\n",
    "        air = spark.table(src)\n",
    "        if last_load:\n",
    "            # TODO: Update this to use transform_ts from Silver Air Hourly\n",
    "            air = air.filter(F.col(\"load_timestamp\") > F.lit(last_load))\n",
    "        # derive date \n",
    "        air = air.withColumn(\"data_timestamp\", F.to_date(\"data_timestamp\")) \\\n",
    "        # aggregations \n",
    "        # Exclude T.LongType and T.IntegerType to ensure dimkey and aqi value are omitted\n",
    "        numeric_cols = [f.name for f in air.schema.fields if isinstance(f.dataType, (T.FloatType, T.DoubleType, T.DecimalType))]\n",
    "\n",
    "        agg_exprs = []\n",
    "        # aggregate stats for each numeric column: mean/max/min\n",
    "        for c in numeric_cols:\n",
    "            agg_exprs.extend([\n",
    "                F.avg(F.col(c)).alias(f\"{c}_mean\"),\n",
    "                F.max(F.col(c)).alias(f\"{c}_max\"),\n",
    "                F.min(F.col(c)).alias(f\"{c}_min\"),\n",
    "            ])\n",
    "        # --- ADD: max of load_timestamp and dimkey ---\n",
    "        agg_exprs.append(F.max(F.col(\"load_timestamp\")).alias(\"max_load_timestamp\"))\n",
    "        agg_exprs.append(F.max(F.col(\"dim_key\")).alias(\"dim_key\"))\n",
    "\n",
    "        # --- ADD: Unhealhy hours in day with AQI > 200  ---\n",
    "        agg_exprs.append(F.sum(F.when(F.col(\"aqi_value\") >= 200, 1).otherwise(0)).alias(\"unhealthy_hours\"))\n",
    "\n",
    "        daily = air.groupBy(\"city\", \"data_timestamp\").agg(*agg_exprs)\n",
    "\n",
    "        # print(daily.head(5))\n",
    "        # Add other required columns\n",
    "        # print(\"Before columns additon\")\n",
    "        daily = daily.withColumn(\"transform_ts\", F.current_timestamp()) \\\n",
    "            .withColumn(\"run_id\", F.lit(run_id))\n",
    "\n",
    "        #ADD AQI\n",
    "        daily = compute_aqi_index_and_label(daily)\n",
    "        \n",
    "        #print(\"Before merge function call\")\n",
    "        recs, min_dt, max_dt, max_lt = merge_into_silver(silver_air_daily_tbl, daily\n",
    "                                                         , merge_keys=(\"city\", \"data_timestamp\"))\n",
    "        \n",
    "        total_processed += recs\n",
    "        if min_dt:\n",
    "            agg_min_data_ts = min_dt if agg_min_data_ts is None else min(agg_min_data_ts, min_dt)\n",
    "        if max_dt:\n",
    "            agg_max_data_ts = max_dt if agg_max_data_ts is None else max(agg_max_data_ts, max_dt)\n",
    "\n",
    "        # commit meta only after successful merge\n",
    "        if max_lt:\n",
    "            commit_last_processed_load_ts(src, max_lt)\n",
    "        print(\"\uD83C\uDF24️ Daily air aggregates updated.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR while building {silver_air_daily_tbl}: {str(e)}\")\n",
    "    update_pipeline_log(\"FAILED\", f\"Air daily aggregation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c2bf35-445f-4fcf-8240-80cde8fc3137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ✅ Step 9: Finalize and Log Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c7bcb70-04fb-4e5c-a0bf-e4de9cab93d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ pipeline_log updated: SUCCESS\n✅ Silver layer completed successfully. Rows processed: 0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    update_pipeline_log(\"SUCCESS\", \"Silver layer completed successfully\", total_processed, agg_min_data_ts, agg_max_data_ts)\n",
    "    print(f\"✅ Silver layer completed successfully. Rows processed: {total_processed}\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Final log update failed:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}