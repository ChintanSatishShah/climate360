{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "371572bf-3510-44b5-b4c2-5ddd52fe17af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# \uD83E\uDDF1 DATABRICKS HACKATHON PROJECT\n",
    "# ==============================================================\n",
    "# NOTEBOOK: 01_Historical_Weather_AirQuality_Ingestion\n",
    "# PURPOSE:  Fetch historical weather & air quality data from Open-Meteo API \n",
    "#           for all active cities in City Master, and store in Bronze layer.\n",
    "# AUTHOR:   Chintan Shah\n",
    "# =============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79a15a44-14b2-4921-8a6c-0396510df30d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83C\uDF0D Weather & Air Quality Bronze Layer\n",
    "\n",
    "### Notebook: `01_Batch_Ingestion`\n",
    "\n",
    "Unified ingestion pipeline for **Weather + Air Quality** data using the **Open-Meteo APIs**.\n",
    "\n",
    "- Automatically creates Delta tables if missing  \n",
    "- Fetches hourly and daily data for all active cities  \n",
    "- Logs each run in `pipeline_log`  \n",
    "- Uses caching and retry for API reliability  \n",
    "\n",
    "**References**\n",
    "- [Weather API Docs](https://open-meteo.com/en/docs)\n",
    "- [Air Quality API Docs](https://open-meteo.com/en/docs/air-quality-api)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f5658a2-593e-492b-9fb1-2aa0bb49c717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1️⃣ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c50f7aa7-a7db-41d4-875b-8182cc477269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openmeteo_requests in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8fb7f2be-8abd-4069-95aa-277ebcaf1015/lib/python3.12/site-packages (1.7.4)\nRequirement already satisfied: niquests>=3.15.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8fb7f2be-8abd-4069-95aa-277ebcaf1015/lib/python3.12/site-packages (from openmeteo_requests) (3.15.2)\nRequirement already satisfied: openmeteo-sdk>=1.22.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8fb7f2be-8abd-4069-95aa-277ebcaf1015/lib/python3.12/site-packages (from openmeteo_requests) (1.23.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from niquests>=3.15.2->openmeteo_requests) (3.3.2)\nRequirement already satisfied: urllib3-future<3,>=2.13.903 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8fb7f2be-8abd-4069-95aa-277ebcaf1015/lib/python3.12/site-packages (from niquests>=3.15.2->openmeteo_requests) (2.14.906)\nRequirement already satisfied: wassima<3,>=1.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8fb7f2be-8abd-4069-95aa-277ebcaf1015/lib/python3.12/site-packages (from niquests>=3.15.2->openmeteo_requests) (2.0.2)\nRequirement already satisfied: flatbuffers==25.9.23 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8fb7f2be-8abd-4069-95aa-277ebcaf1015/lib/python3.12/site-packages (from openmeteo-sdk>=1.22.0->openmeteo_requests) (25.9.23)\nRequirement already satisfied: h11<1.0.0,>=0.11.0 in /databricks/python3/lib/python3.12/site-packages (from urllib3-future<3,>=2.13.903->niquests>=3.15.2->openmeteo_requests) (0.14.0)\nRequirement already satisfied: jh2<6.0.0,>=5.0.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8fb7f2be-8abd-4069-95aa-277ebcaf1015/lib/python3.12/site-packages (from urllib3-future<3,>=2.13.903->niquests>=3.15.2->openmeteo_requests) (5.0.10)\nRequirement already satisfied: qh3<2.0.0,>=1.5.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8fb7f2be-8abd-4069-95aa-277ebcaf1015/lib/python3.12/site-packages (from urllib3-future<3,>=2.13.903->niquests>=3.15.2->openmeteo_requests) (1.5.6)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: requests_cache in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8fb7f2be-8abd-4069-95aa-277ebcaf1015/lib/python3.12/site-packages (1.2.1)\nRequirement already satisfied: attrs>=21.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8fb7f2be-8abd-4069-95aa-277ebcaf1015/lib/python3.12/site-packages (from requests_cache) (25.4.0)\nRequirement already satisfied: cattrs>=22.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8fb7f2be-8abd-4069-95aa-277ebcaf1015/lib/python3.12/site-packages (from requests_cache) (25.3.0)\nRequirement already satisfied: platformdirs>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests_cache) (3.10.0)\nRequirement already satisfied: requests>=2.22 in /databricks/python3/lib/python3.12/site-packages (from requests_cache) (2.32.3)\nRequirement already satisfied: url-normalize>=1.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8fb7f2be-8abd-4069-95aa-277ebcaf1015/lib/python3.12/site-packages (from requests_cache) (2.2.1)\nRequirement already satisfied: urllib3>=1.25.5 in /databricks/python3/lib/python3.12/site-packages (from requests_cache) (2.3.0)\nRequirement already satisfied: typing-extensions>=4.14.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8fb7f2be-8abd-4069-95aa-277ebcaf1015/lib/python3.12/site-packages (from cattrs>=22.2->requests_cache) (4.15.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.22->requests_cache) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.22->requests_cache) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.22->requests_cache) (2025.1.31)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: retry_requests in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8fb7f2be-8abd-4069-95aa-277ebcaf1015/lib/python3.12/site-packages (2.0.0)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from retry_requests) (2.32.3)\nRequirement already satisfied: urllib3>=1.26 in /databricks/python3/lib/python3.12/site-packages (from retry_requests) (2.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->retry_requests) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->retry_requests) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->retry_requests) (2025.1.31)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages, and restart the Python kernel\n",
    "# Can add it to environment init script\n",
    "%pip install openmeteo_requests\n",
    "%pip install requests_cache\n",
    "%pip install retry_requests\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54e8bf30-a635-4516-a18e-c9d7822b32b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping CDF config (not supported in this workspace): [CONFIG_NOT_AVAILABLE] Configuration spark.databricks.delta.properties.defaults.enableChangeDataFeed is not available. SQLSTATE: 42K0I;\nSetCommand (spark.databricks.delta.properties.defaults.enableChangeDataFeed,Some(true))\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat com.databricks.sql.connect.SparkConnectConfig$.assertConfigAllowed(SparkConnectConfig.scala:284)\n\tat com.databricks.sql.connect.SparkConnectSetFilteringValidationCheck.$anonfun$apply$1(SparkConnectSetFilteringValidationCheck.scala:33)\n\tat com.databricks.sql.connect.SparkConnectSetFilteringValidationCheck.$anonfun$apply$1$adapted(SparkConnectSetFilteringValidationCheck.scala:27)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:302)\n\tat com.databricks.sql.connect.SparkConnectSetFilteringValidationCheck.apply(SparkConnectSetFilteringValidationCheck.scala:27)\n\tat com.databricks.sql.connect.SparkConnectSetFilteringValidationCheck.apply(SparkConnectSetFilteringValidationCheck.scala:25)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$62(CheckAnalysis.scala:1045)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$62$adapted(CheckAnalysis.scala:1045)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:1045)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:266)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:262)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:439)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:439)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:639)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:639)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:631)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:347)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:157)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:825)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:821)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:809)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:819)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:329)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:328)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:389)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:302)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:154)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:856)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:819)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3586)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3410)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3287)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:186)\n\tat scala.Option.flatMap(Option.scala:283)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:186)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:832)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:779)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:249)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:237)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:215)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:215)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:130)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:130)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:112)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:112)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, min as spark_min, max as spark_max\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "try:\n",
    "    spark.sql(\"SET spark.databricks.delta.properties.defaults.enableChangeDataFeed = true\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Skipping CDF config (not supported in this workspace): {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8623ac3-1458-4f56-b1f5-03aef2686fc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2️⃣ Parameters and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba16656a-90da-4a11-8916-263d0f0f9901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing HISTORICAL data for cities from 2025-02-01 to 2025-02-01\n"
     ]
    }
   ],
   "source": [
    "# --- Pipeline context ---\n",
    "pipeline_name = \"BATCH_INGEST\"\n",
    "# TODO: Determine if Manual run or Scheduled run\n",
    "triggered_by = \"Manual\"   # Can be Scheduled/Event driven if automated\n",
    "\n",
    "# Set defaults\n",
    "run_type = \"HOURLY\"   # Change to HISTORICAL / DAILY / 15MIN as per notebook context\n",
    "weather_url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "# Define a default (yesterday → today+6 days for example)\n",
    "today_utc = datetime.now(timezone.utc).date()\n",
    "default_start = (today_utc - timedelta(days=1))\n",
    "default_end   = (today_utc + timedelta(days=6))\n",
    "\n",
    "# Get widget values safely\n",
    "start_date_str = dbutils.widgets.get(\"start_date\") or default_start.strftime(\"%Y-%m-%d\")\n",
    "end_date_str   = dbutils.widgets.get(\"end_date\") or default_end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Validate & parse\n",
    "try:\n",
    "    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\").date()\n",
    "    end_date   = datetime.strptime(end_date_str, \"%Y-%m-%d\").date()\n",
    "    if start_date < default_start: \n",
    "        run_type = \"HISTORICAL\" \n",
    "        weather_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "        if end_date > today_utc: \n",
    "            print(f\"⚠️ Adjusting end_date from {end_date} → {today} (API historical limit)\")\n",
    "            end_date = today_utc.strftime(\"%Y-%m-%d\")\n",
    "except ValueError:\n",
    "    raise ValueError(f\"Invalid start_date or end_date format. Must be YYYY-MM-DD. Got: {start_date_str}, {end_date_str}\")\n",
    "\n",
    "# end_date >= start_date Validation\n",
    "assert end_date >= start_date, \"Validation Error: end_date must be greater than or equal to start_date\"\n",
    "\n",
    "print(f\"Processing {run_type} data for cities from {start_date} to {end_date}\")\n",
    "\n",
    "catalog = \"env_catalog\"\n",
    "schema = \"env_data\"\n",
    "\n",
    "city_master_tbl = f\"{catalog}.{schema}.city_master\"\n",
    "pipeline_log_tbl = f\"{catalog}.{schema}.pipeline_log\"\n",
    "\n",
    "weather_hourly_tbl = f\"{catalog}.{schema}.bronze_weather_hourly\"\n",
    "weather_daily_tbl = f\"{catalog}.{schema}.bronze_weather_daily\"\n",
    "air_hourly_tbl = f\"{catalog}.{schema}.bronze_air_hourly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6087218f-a57f-47ae-ac49-d34c572da0bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3️⃣ Setup Open-Meteo Client (Cache + Retry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d20c1b45-d0c5-4dae-a631-0389020e0a24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cache_session = requests_cache.CachedSession(\".cache\", expire_after=3600*6) # 6 hours\n",
    "retry_session = retry(cache_session, retries=3, backoff_factor=0.5)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64931d17-2cf8-42fd-b8f9-84afdfb9c4db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4️⃣ Load City Master (Active Cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f13dd1c-22db-445e-8356-c2afec3cd48d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 2 active cities to process\n"
     ]
    }
   ],
   "source": [
    "city_df = spark.table(city_master_tbl).filter(\"is_active = true and is_latest = true\").toPandas()\n",
    "cities_dict = city_df.to_dict(\"records\")\n",
    "print(f\"✅ Found {len(cities_dict)} active cities to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca61b0f-54b9-48ab-91a2-7d51b482f445",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5️⃣ Pipeline Logging — Capture Run Start, Completion, and Status\n",
    "\n",
    "This section wraps the ingestion logic with automatic pipeline logging.\n",
    "- Creates a structured `run_id` based on pipeline type and ingestion timestamp  \n",
    "- Inserts a \"RUNNING\" entry into the `pipeline_log`  \n",
    "- Updates the log with `SUCCESS` or `FAILED` at the end  \n",
    "- Captures record counts and earliest/latest timestamps from the data  \n",
    "- Ensures consistent auditability across Historical, Daily, and 15-min pipelines  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39a72271-44af-4bba-b69a-0c30105ac402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD39 Generated run_id: HISTORICAL_BATCH_INGEST_20251112_124744\n✅ Pipeline started: HISTORICAL_BATCH_INGEST_20251112_124744\n"
     ]
    }
   ],
   "source": [
    "# --- Generate run_id with current timestamp for readability ---\n",
    "start_time = datetime.now(timezone.utc)\n",
    "run_id = f\"{run_type}_{pipeline_name.upper()}_{start_time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "print(f\"\uD83D\uDD39 Generated run_id: {run_id}\")\n",
    "\n",
    "# --- Log the start of the run ---\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO env_catalog.env_data.pipeline_log\n",
    "(run_id, pipeline_name, run_type, start_time, status, triggered_by, created_ts)\n",
    "VALUES ('{run_id}', '{pipeline_name}', '{run_type}', TIMESTAMP '{start_time}', 'RUNNING', '{triggered_by}', CURRENT_TIMESTAMP())\n",
    "\"\"\")\n",
    "\n",
    "print(f\"✅ Pipeline started: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe91ae2f-7b66-4d99-b8f2-b6bd118c96de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6️⃣ Define API Parameter Lists (Weather + Air Quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3009de67-78d6-4d30-818b-3ec9b4da6059",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_hourly_params = [\n",
    "    \"weather_code\", \n",
    "    \"temperature_2m\", \"apparent_temperature\", \"relative_humidity_2m\",\n",
    "    \"pressure_msl\", \"surface_pressure\",\n",
    "    \"wind_speed_10m\", \"wind_gusts_10m\", \"wind_direction_10m\",\n",
    "    \"cloud_cover\", \"visibility\",\n",
    "    \"precipitation\", \"rain\", \"showers\", \"snowfall\",\n",
    "    \"uv_index\", \"uv_index_clear_sky\", \"is_day\", \n",
    "]\n",
    "\n",
    "weather_daily_params = [\n",
    "    \"weather_code\",\n",
    "    \"temperature_2m_mean\", \"temperature_2m_max\", \"temperature_2m_min\",\n",
    "    \"apparent_temperature_mean\", \"apparent_temperature_max\", \"apparent_temperature_min\",\n",
    "    \"relative_humidity_2m_mean\", \"relative_humidity_2m_max\", \"relative_humidity_2m_min\",\n",
    "    \"pressure_msl_mean\", \"pressure_msl_max\", \"pressure_msl_min\",\n",
    "    \"surface_pressure_mean\", \"surface_pressure_max\", \"surface_pressure_min\",\n",
    "    \"wind_speed_10m_max\", \"wind_gusts_10m_max\", \"wind_direction_10m_dominant\",\n",
    "    \"cloud_cover_mean\", \"cloud_cover_max\", \"cloud_cover_min\",\n",
    "    \"visibility_mean\", \"visibility_max\", \"visibility_min\",\n",
    "    \"precipitation_hours\", \"precipitation_sum\", \"rain_sum\", \"showers_sum\", \"snowfall_sum\",\n",
    "    \"uv_index_max\", \"uv_index_clear_sky_max\",\n",
    "    \"sunrise\", \"sunset\", \"daylight_duration\", \"sunshine_duration\"\n",
    "]\n",
    "\n",
    "air_quality_params = [\n",
    "    \"pm10\", \"pm2_5\", \"carbon_monoxide\", \"carbon_dioxide\",\n",
    "    \"nitrogen_monoxide\", \"nitrogen_dioxide\", \"sulphur_dioxide\",\n",
    "    \"ozone\", \"aerosol_optical_depth\", \"dust\",\n",
    "    \"uv_index\", \"uv_index_clear_sky\", \"ammonia\", \"methane\",\n",
    "    \"european_aqi\", \"us_aqi\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d619067-bb37-4a5a-aaf8-6623f446da4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7️⃣ Helper function to create delta tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "920a4ab1-4476-41c6-aa26-2680d880c3fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_table_if_missing(table_name: str, df_spark, partition_col=\"city\"):\n",
    "    \"\"\"\n",
    "    If table doesn't exist, create it using the dataframe schema and partition by partition_col.\n",
    "    \"\"\"\n",
    "    if spark.catalog.tableExists(table_name):\n",
    "        print(f\"✅ Table {table_name} exists\")\n",
    "    # except AnalysisException:\n",
    "    else:\n",
    "        print(f\"⚙️ Creating table {table_name} (partitioned by {partition_col})...\")\n",
    "        # create table with partitioning\n",
    "        # Append Only\n",
    "        # df_spark.write.format(\"delta\").option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "        #     .partitionBy(partition_col).saveAsTable(table_name)\n",
    "        # Merge supported\n",
    "        df_spark.write.format(\"delta\").option(\"overwriteSchema\", \"true\") \\\n",
    "            .partitionBy(partition_col).mode(\"overwrite\").saveAsTable(table_name)\n",
    "        print(f\"✅ Created detla table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e99241-ae14-43e7-981f-0fc3a218688c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8️⃣ Function to run the entire loading process in a try-catch loop\n",
    "\n",
    "\n",
    "### \uD83D\uDD39 Timestamp Design (Reference)\n",
    "\n",
    "| Column | Meaning | Source |\n",
    "|---------|----------|--------|\n",
    "| `data_timestamp` | Actual time of the event / reading from source (e.g. API timestamp). Used for joins | Source system |\n",
    "| `load_timestamp` | Ingestion time into Databricks (Bronze → Silver → Gold). Used for auditing, backfills, and debugging and incremental load to next layers | Current UTC timestamp (`current_timestamp()` or `datetime.now(timezone.utc)`) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33484fd5-b1d1-4806-bf3f-751853fa4644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fetch_and_process_data(cities, start_date, end_date, cache):\n",
    "    weather_hourly_all, weather_daily_all, aq_hourly_all = [], [], []\n",
    "\n",
    "    # API Calls (One per dataset for all cities)\n",
    "    weather_resp = openmeteo.weather_api(weather_url,\n",
    "        params={\n",
    "            \"latitude\": [c[\"latitude\"] for c in cities],\n",
    "            \"longitude\": [c[\"longitude\"] for c in cities],\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"hourly\": weather_hourly_params,\n",
    "            \"daily\": weather_daily_params,\n",
    "            \"timezone\": \"GMT\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    air_resp = openmeteo.weather_api(\n",
    "        \"https://air-quality-api.open-meteo.com/v1/air-quality\",\n",
    "        params={\n",
    "            \"latitude\": [c[\"latitude\"] for c in cities],\n",
    "            \"longitude\": [c[\"longitude\"] for c in cities],\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"hourly\": air_quality_params,\n",
    "            \"timezone\": \"GMT\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Process API Responses into Pandas DataFrames\n",
    "    for i, city_info in enumerate(cities):\n",
    "        city = city_info[\"city_name\"]\n",
    "        dim_key = city_info[\"dim_key\"]\n",
    "        \n",
    "        # Hourly\n",
    "        hourly = weather_resp[i].Hourly()\n",
    "        hourly_df = {\n",
    "            \"dim_key\": dim_key,\n",
    "            \"city\": city,\n",
    "            \"run_id\": run_id,\n",
    "            \"load_timestamp\": datetime.now(timezone.utc),\n",
    "            \"data_timestamp\": pd.date_range(\n",
    "                start = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "                end =  pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "                freq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "                inclusive = \"left\"\n",
    "            )\n",
    "        }\n",
    "        #print(hourly_df)\n",
    "        for index, param in enumerate(weather_hourly_params):\n",
    "            var_obj = hourly.Variables(index)\n",
    "            try:\n",
    "                values = var_obj.ValuesAsNumpy()\n",
    "            except ValueError:\n",
    "                # Fallback for categorical/string variables\n",
    "                values = np.array(var_obj.Values(), dtype=object)\n",
    "            hourly_df[param] = values\n",
    "        weather_hourly_all.append(pd.DataFrame(hourly_df))\n",
    "        #print(weather_hourly_all)\n",
    "        \n",
    "        # # Daily\n",
    "        daily = weather_resp[i].Daily()\n",
    "        daily_df = {\n",
    "            \"dim_key\": dim_key,\n",
    "            \"city\": city,\n",
    "            \"run_id\": run_id,\n",
    "            \"load_timestamp\": datetime.now(timezone.utc),\n",
    "            \"data_timestamp\": pd.date_range(\n",
    "                start = pd.to_datetime(daily.Time(), unit = \"s\", utc = True),\n",
    "                end =  pd.to_datetime(daily.TimeEnd(), unit = \"s\", utc = True),\n",
    "                freq = pd.Timedelta(seconds = daily.Interval()),\n",
    "                inclusive = \"left\"\n",
    "            )\n",
    "        }\n",
    "        #print(daily_df)\n",
    "        for index, param in enumerate(weather_daily_params):\n",
    "            var_obj = daily.Variables(index)\n",
    "            try:\n",
    "                values = var_obj.ValuesAsNumpy()\n",
    "            except ValueError:\n",
    "                # Fallback for categorical/string variables\n",
    "                values = np.array(var_obj.Values(), dtype=object)\n",
    "            daily_df[param] = values\n",
    "        weather_daily_all.append(pd.DataFrame(daily_df))\n",
    "        #print(weather_daily_all)\n",
    "\n",
    "        # # Air Quality\n",
    "        aq_hourly = air_resp[i].Hourly()\n",
    "        aq_hourly_df = {\n",
    "            \"dim_key\": dim_key,\n",
    "            \"city\": city,\n",
    "            \"run_id\": run_id,\n",
    "            \"load_timestamp\": datetime.now(timezone.utc),\n",
    "            \"data_timestamp\": pd.date_range(\n",
    "                start = pd.to_datetime(aq_hourly.Time(), unit = \"s\", utc = True),\n",
    "                end =  pd.to_datetime(aq_hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "                freq = pd.Timedelta(seconds = aq_hourly.Interval()),\n",
    "                inclusive = \"left\"\n",
    "            )\n",
    "        }\n",
    "        #print(aq_hourly_df)\n",
    "        for index, param in enumerate(air_quality_params):\n",
    "            var_obj = aq_hourly.Variables(index)\n",
    "            try:\n",
    "                values = var_obj.ValuesAsNumpy()\n",
    "            except ValueError:\n",
    "                # Fallback for categorical/string variables\n",
    "                values = np.array(var_obj.Values(), dtype=object)\n",
    "            aq_hourly_df[param] = values\n",
    "        aq_hourly_all.append(pd.DataFrame(aq_hourly_df))\n",
    "        #print(aq_hourly_all)\n",
    "\n",
    "    weather_hourly_df = pd.concat(weather_hourly_all)\n",
    "    weather_daily_df = pd.concat(weather_daily_all)\n",
    "    air_hourly_df = pd.concat(aq_hourly_all)\n",
    "\n",
    "    return weather_hourly_df, weather_daily_df, air_hourly_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad3c1d4-947f-48a1-98fc-ee625c67dc03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9️⃣ Function to merge the new rows in bronze table\n",
    "\n",
    "\uD83D\uDD01 Idempotent Merge for Bronze Tables (City + Timestamp/Date)\n",
    "\n",
    "This replaces append writes with Delta `MERGE` (upsert).\n",
    "- Matches on (`city`, `data_timestamp`)\n",
    "- Updates all columns when matched, inserts when not matched.\n",
    "- Automatically creates the target Delta table if it doesn’t exist.\n",
    "- Updates pipeline_log with `records_processed`, `earliest_ts`, `latest_ts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01c098b5-f1c5-46e7-b26c-162233f93571",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_from_sdf(target_table, staging_sdf, merge_keys, exclude_update_cols=None):\n",
    "    \"\"\"\n",
    "    Merge staging_sdf into target_table on merge_keys.\n",
    "    Returns: (rows_processed, earliest_ts, latest_ts)\n",
    "    \"\"\"\n",
    "    if staging_sdf is None:\n",
    "        print(f\"Skipping merge for {target_table} - no staging dataframe.\")\n",
    "        return 0, None, None\n",
    "\n",
    "    # normalize timestamp/date types if present\n",
    "    if \"load_timestamp\" in staging_sdf.columns:\n",
    "        staging_sdf = staging_sdf.withColumn(\"load_timestamp\", F.to_timestamp(F.col(\"load_timestamp\")))\n",
    "    if \"data_timestamp\" in staging_sdf.columns:\n",
    "        staging_sdf = staging_sdf.withColumn(\"data_timestamp\", F.to_timestamp(F.col(\"data_timestamp\")))\n",
    "    \n",
    "    # if \"date\" in staging_sdf.columns:\n",
    "    #     # cast string dates to date if required\n",
    "    #     if dict(staging_sdf.dtypes).get(\"date\") == \"string\":\n",
    "    #         staging_sdf = staging_sdf.withColumn(\"date\", F.to_date(F.col(\"date\")))\n",
    "\n",
    "    # ensure target exists\n",
    "    create_table_if_missing(target_table, staging_sdf, partition_col=merge_keys[0])\n",
    "    print(\"Bronze tables found/created\")\n",
    "    \n",
    "    # create staging temp view (unique per run)\n",
    "    safe_run = (run_id if 'run_id' in globals() else str(np.random.randint(1e9))).replace(\"-\", \"_\")\n",
    "    tmp_view = f\"stg_{target_table.replace('.', '_')}_{safe_run}\"\n",
    "    staging_sdf.createOrReplaceTempView(tmp_view)\n",
    "    print(\"Staging temp view created\")\n",
    "    \n",
    "    cols = staging_sdf.columns\n",
    "    for k in merge_keys:\n",
    "        if k not in cols:\n",
    "            raise ValueError(f\"Merge key '{k}' not found in staging columns: {cols}\")\n",
    "\n",
    "    exclude_update_cols = exclude_update_cols or []\n",
    "    update_cols = [c for c in cols if c not in merge_keys + exclude_update_cols]\n",
    "    if not update_cols:\n",
    "        raise ValueError(\"No updateable columns available (all are keys or excluded).\")\n",
    "\n",
    "    on_clause = \" AND \".join([f\"t.`{k}` = s.`{k}`\" for k in merge_keys])\n",
    "    update_set = \", \".join([f\"t.`{c}` = s.`{c}`\" for c in update_cols])\n",
    "    insert_cols = \", \".join([f\"`{c}`\" for c in cols])\n",
    "    insert_vals = \", \".join([f\"s.`{c}`\" for c in cols])\n",
    "\n",
    "    merge_sql = f\"\"\"\n",
    "      MERGE INTO {target_table} t\n",
    "      USING {tmp_view} s\n",
    "      ON {on_clause}\n",
    "      WHEN MATCHED THEN UPDATE SET {update_set}\n",
    "      WHEN NOT MATCHED THEN INSERT ({insert_cols}) VALUES ({insert_vals})\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Executing MERGE INTO {target_table} ...\")\n",
    "    spark.sql(merge_sql)\n",
    "    rowcount = staging_sdf.count()\n",
    "\n",
    "    # compute earliest/latest ts if any candidate present\n",
    "    ts_col = next((c for c in (\"data_timestamp\", \"date\", \"time\") if c in cols), None)\n",
    "    earliest_ts = latest_ts = None\n",
    "    if ts_col:\n",
    "        agg = staging_sdf.agg(F.min(F.col(ts_col)).alias(\"min_ts\"), F.max(F.col(ts_col)).alias(\"max_ts\")).collect()[0]\n",
    "        earliest_ts, latest_ts = agg[\"min_ts\"], agg[\"max_ts\"]\n",
    "\n",
    "    print(f\"MERGE done for {target_table} — staged_rows={rowcount}, earliest_ts={earliest_ts}, latest_ts={latest_ts}\")\n",
    "    return rowcount, earliest_ts, latest_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "217e3899-7393-4bc0-ad31-067469d39209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83D\uDD1F Execute Data Ingestion with Logging Control\n",
    "Wrap the full ingestion process inside a try/except/finally block  \n",
    "so any failures are properly logged in the `pipeline_log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15640dc1-207d-43e4-9749-3e6b62f0b490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Creating table env_catalog.env_data.bronze_weather_hourly (partitioned by city)...\n✅ Created detla table: env_catalog.env_data.bronze_weather_hourly\nBronze tables found/created\nStaging temp view created\nExecuting MERGE INTO env_catalog.env_data.bronze_weather_hourly ...\nMERGE done for env_catalog.env_data.bronze_weather_hourly — staged_rows=48, earliest_ts=2025-02-01 00:00:00, latest_ts=2025-02-01 23:00:00\n⚙️ Creating table env_catalog.env_data.bronze_weather_daily (partitioned by city)...\n✅ Created detla table: env_catalog.env_data.bronze_weather_daily\nBronze tables found/created\nStaging temp view created\nExecuting MERGE INTO env_catalog.env_data.bronze_weather_daily ...\nMERGE done for env_catalog.env_data.bronze_weather_daily — staged_rows=2, earliest_ts=2025-02-01 00:00:00, latest_ts=2025-02-01 00:00:00\n⚙️ Creating table env_catalog.env_data.bronze_air_hourly (partitioned by city)...\n✅ Created detla table: env_catalog.env_data.bronze_air_hourly\nBronze tables found/created\nStaging temp view created\nExecuting MERGE INTO env_catalog.env_data.bronze_air_hourly ...\nMERGE done for env_catalog.env_data.bronze_air_hourly — staged_rows=48, earliest_ts=2025-02-01 00:00:00, latest_ts=2025-02-01 23:00:00\nTotal staged records processed: 98  earliest=2025-02-01 00:00:00  latest=2025-02-01 23:00:00\n\uD83C\uDFAF Run HISTORICAL_BATCH_INGEST_20251112_124744 completed successfully for 2 cities between 2025-02-01 and 2025-02-01.\n\uD83C\uDFC1 Pipeline SUCCESS: HISTORICAL_BATCH_INGEST_20251112_124744\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # -----------------------------------------------\n",
    "    # \uD83C\uDF00 Main Ingestion Logic — Weather + Air Quality\n",
    "    # -----------------------------------------------\n",
    "    weather_df_hourly, weather_df_daily, air_df_hourly = fetch_and_process_data(\n",
    "        cities=cities_dict,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        cache=cache_session\n",
    "    )\n",
    "\n",
    "    # Auto-Create Bronze Delta Tables (If Not Exists)\n",
    "    # create_table_if_missing(weather_hourly_tbl, weather_df_hourly)\n",
    "    # create_table_if_missing(weather_daily_tbl, weather_df_daily)\n",
    "    # create_table_if_missing(air_hourly_tbl, air_df_hourly)\n",
    "    \n",
    "    # Append Data into Delta Tables\n",
    "    # spark.createDataFrame(weather_df_hourly).write.mode(\"append\").format(\"delta\").saveAsTable(weather_hourly_tbl)\n",
    "    # spark.createDataFrame(weather_df_daily).write.mode(\"append\").format(\"delta\").saveAsTable(weather_daily_tbl)\n",
    "    # spark.createDataFrame(air_df_hourly).write.mode(\"append\").format(\"delta\").saveAsTable(air_hourly_tbl)\n",
    "\n",
    "    # Merge Data into Delta Tables\n",
    "    total_records = 0\n",
    "    agg_earliest = None\n",
    "    agg_latest = None\n",
    "\n",
    "    targets = [\n",
    "        (weather_hourly_tbl, spark.createDataFrame(weather_df_hourly),  [\"city\", \"data_timestamp\"]),\n",
    "        (weather_daily_tbl,  spark.createDataFrame(weather_df_daily),    [\"city\", \"data_timestamp\"]),\n",
    "        (air_hourly_tbl,     spark.createDataFrame(air_df_hourly),       [\"city\", \"data_timestamp\"])\n",
    "    ]\n",
    "\n",
    "    for tbl_name, sdf, keys in targets:\n",
    "        recs, min_ts, max_ts = merge_from_sdf(tbl_name, sdf, merge_keys=keys, exclude_update_cols=[])  # optionally exclude ['run_id']\n",
    "        total_records += recs\n",
    "        if min_ts is not None:\n",
    "            agg_earliest = min(min_ts, agg_earliest) if agg_earliest is not None else min_ts\n",
    "        if max_ts is not None:\n",
    "            agg_latest = max(max_ts, agg_latest) if agg_latest is not None else max_ts\n",
    "\n",
    "    print(f\"Total staged records processed: {total_records}  earliest={agg_earliest}  latest={agg_latest}\")\n",
    "\n",
    "    # Cleanup data older than 7 days\n",
    "    spark.sql(f\"\"\"VACUUM {weather_hourly_tbl} RETAIN 168 HOURS\"\"\")\n",
    "    spark.sql(f\"\"\"VACUUM {weather_daily_tbl} RETAIN 168 HOURS\"\"\")    \n",
    "    spark.sql(f\"\"\"VACUUM {air_hourly_tbl} RETAIN 168 HOURS\"\"\")\n",
    "\n",
    "    # --- Mark success ---\n",
    "    status = \"SUCCESS\"\n",
    "    remarks = f\"Successfully loaded {total_records} records across weather & air quality tables.\"\n",
    "    earliest_ts = agg_earliest # datetime.strptime(agg_earliest, \"%Y-%m-%d\")\n",
    "    latest_ts = agg_latest #datetime.strptime(agg_latest, \"%Y-%m-%d\")\n",
    "\n",
    "    print(f\"\uD83C\uDFAF Run {run_id} completed successfully for {len(cities_dict)} cities between {start_date} and {end_date}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    status = \"FAILED\"\n",
    "    remarks = f\"Pipeline failed due to: {str(e)[:200]}\"\n",
    "    total_records = 0\n",
    "    earliest_ts = datetime.strptime('2000-01-01', \"%Y-%m-%d\")\n",
    "    latest_ts = datetime.strptime('2000-01-01', \"%Y-%m-%d\")\n",
    "\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # --- Always log end state ---\n",
    "    end_time = datetime.now(timezone.utc)\n",
    "    spark.sql(f\"\"\"\n",
    "        UPDATE {pipeline_log_tbl}\n",
    "        SET end_time = TIMESTAMP '{end_time}',\n",
    "            status = '{status}',\n",
    "            records_processed = {total_records},\n",
    "            earliest_ts = TIMESTAMP '{earliest_ts}',\n",
    "            latest_ts = TIMESTAMP '{latest_ts}',\n",
    "            remarks = '{remarks}'\n",
    "        WHERE run_id = '{run_id}'\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"\uD83C\uDFC1 Pipeline {status}: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9856c257-ca59-4683-b281-71c7a27bdb51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ✅ Summary\n",
    "| Table | Type | Partition | Notes |\n",
    "|--------|-------|------------|--------|\n",
    "| `bronze_weather_hourly` | Weather (Hourly) | city | From Open-Meteo Archive / Weather API |\n",
    "| `bronze_weather_daily` | Weather (Daily) | city | From Open-Meteo Archive / Weather API |\n",
    "| `bronze_air_hourly` | Air Quality (Hourly) | city | From Air Quality API |\n",
    "\n",
    "**Further Additons Planned:**  \n",
    "- Create an autoloader based ingestion for past and next 24 hours data with load to volumes at 15 min.\n",
    "- However, except for North America and Central Europe data is interpolated for 15 min. Hence is not much useful\n",
    "- Autoloader can be executed separately \n",
    "- Add unit tests / validation "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Batch_Ingestion",
   "widgets": {
    "end_date": {
     "currentValue": "2025-02-01",
     "nuid": "c119619e-b8a7-4b0f-8dcc-42615c422d65",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "End Date (YYYY-MM-DD)",
      "name": "end_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "End Date (YYYY-MM-DD)",
      "name": "end_date",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "start_date": {
     "currentValue": "2025-02-01",
     "nuid": "3777171b-f811-4ad6-a48b-b1762e5adca5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Start Date (YYYY-MM-DD)",
      "name": "start_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Start Date (YYYY-MM-DD)",
      "name": "start_date",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}