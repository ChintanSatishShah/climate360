{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3177aaba-cb64-443c-936b-d3cd07a36b93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# \uD83E\uDDF1 DATABRICKS HACKATHON PROJECT\n",
    "# ==============================================================\n",
    "# NOTEBOOK: 00 Environment Setup & Master Data\n",
    "# PURPOSE:  Initializes the environment, creates foundational tables, and seeds initial data \n",
    "# AUTHOR:   Chintan Shah\n",
    "# =============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36e81dcb-830e-43f7-9ff6-c8e58075598b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83E\uDDF1 Setup & Master Data\n",
    "\n",
    "This notebook initializes the environment, creates foundational tables,\n",
    "and seeds initial data for the project.\n",
    "It includes:\n",
    "  1. Catalog & Schema setup\n",
    "  2. Volume creation\n",
    "  3. City Master (dimension table)\n",
    "  4. Run configuration\n",
    "  5. Pipeline logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12ff4ad4-120a-441a-84ba-1bd5a8c9aa2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83E\uDDED Step 1 – Environment Initialization\n",
    "\n",
    "In this step:\n",
    "- Create a **Catalog** and **Schema** to organize project assets.\n",
    "- Set the active context for all operations.\n",
    "\n",
    "Schema: `env_data`  \n",
    "Catalog: `env_catalog`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6534df1-f203-4cc3-9fbd-1f43a7b7db89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment initialized: Catalog = env_catalog, Schema = env_data\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "catalog_name = \"env_catalog\"\n",
    "schema_name = \"env_data\"\n",
    "\n",
    "# Create catalog & schema if not already present\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name} COMMENT 'Schema for Weather + Pollution Data'\")\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_name}\")\n",
    "\n",
    "print(f\"✅ Environment initialized: Catalog = {catalog_name}, Schema = {schema_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f12b18be-ff5a-43ec-8a65-d9f74e52cff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83D\uDCE6 Step 2 – Volume Setup\n",
    "\n",
    "Volumes serve as **managed storage zones** for raw and processed data.  \n",
    "We’ll create:\n",
    "- Landing zones for **weather** and **pollution** data (daily/nightly & 15-min feeds)\n",
    "- A **checkpoint** volume for streaming job state tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef81d2d3-db92-49a8-92c1-76efefae9ea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All volumes created successfully.\n"
     ]
    }
   ],
   "source": [
    "volumes = {\n",
    "    \"weather_landing\": \"Landing zone for raw weather API responses\",\n",
    "    \"pollution_landing\": \"Landing zone for raw pollution API responses\",\n",
    "    \"weather_live\": \"Landing zone for near-live 15-min weather data\",\n",
    "    \"pollution_live\": \"Landing zone for near-live 15-min pollution data\",\n",
    "    \"checkpoints\": \"Checkpoint locations for Autoloader and streaming jobs\"\n",
    "}\n",
    "\n",
    "# Create each volume with comments\n",
    "for vol, comment in volumes.items():\n",
    "    spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog_name}.{schema_name}.{vol} COMMENT '{comment}'\")\n",
    "\n",
    "print(\"✅ All volumes created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "707dd41d-8b83-4393-8a88-6673df80992d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDF0D Step 3 – City Master Dimension Table\n",
    "\n",
    "**Purpose:** Maintain a controlled, versioned list of cities.  \n",
    "\n",
    "**Design Highlights:**\n",
    "- Surrogate key (`dim_key`) generated automatically  \n",
    "- Insert-only: prevents updates/deletes to preserve history  \n",
    "- `is_latest` flag → latest record if city details (lat/long) ever change  \n",
    "- `active_flag` → determines which cities participate in regular ingestion  \n",
    "- `effective_from` / `effective_to` → record validity window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04657f15-dfd0-4ba2-8835-49dd37122bff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ City master table created successfully with defaults enabled.\n"
     ]
    }
   ],
   "source": [
    "table_name = f\"{catalog_name}.{schema_name}.city_master\"\n",
    "\n",
    "# Drop if exists for clean reruns\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "# 1️⃣ Create basic structure (without defaults)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {table_name} (\n",
    "  dim_key BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "  city_name STRING NOT NULL,\n",
    "  latitude DOUBLE,\n",
    "  longitude DOUBLE,\n",
    "  country_code STRING NOT NULL,\n",
    "  country_name STRING,\n",
    "  region STRING,\n",
    "  is_latest BOOLEAN,\n",
    "  is_active BOOLEAN,\n",
    "  effective_from TIMESTAMP,\n",
    "  effective_to TIMESTAMP,\n",
    "  created_ts TIMESTAMP\n",
    ")\n",
    "COMMENT 'Dimension table for City Master (insert-only, version controlled)'\n",
    "\"\"\")\n",
    "\n",
    "# 2️⃣ Enable default support on the table\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {table_name}\n",
    "SET TBLPROPERTIES('delta.feature.allowColumnDefaults' = 'supported')\n",
    "\"\"\")\n",
    "\n",
    "# 3️⃣ Apply default expressions now\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {table_name}\n",
    "ALTER COLUMN is_latest SET DEFAULT TRUE\n",
    "\"\"\")\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {table_name}\n",
    "ALTER COLUMN is_active SET DEFAULT TRUE\n",
    "\"\"\")\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {table_name}\n",
    "ALTER COLUMN effective_from SET DEFAULT current_timestamp()\n",
    "\"\"\")\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {table_name}\n",
    "ALTER COLUMN created_ts SET DEFAULT current_timestamp()\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ City master table created successfully with defaults enabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f18551ac-632e-4290-b2f5-ba1de550e6c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ✨ Load Initial City Master Data\n",
    "\n",
    "Load top **global** and **Indian** cities as seed data.\n",
    "\n",
    "These cities will be the base for all weather & pollution data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f809e56-3df9-4646-a6f0-afe925faaafb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 25 seed cities into city_master.\n"
     ]
    }
   ],
   "source": [
    "# We insert only into user-controlled columns.\n",
    "# Auto-generated / default columns (dim_key, created_ts, etc.) are filled automatically.\n",
    "\n",
    "city_data = [\n",
    "    # --- Global Cities ---\n",
    "    (\"New York\", 40.7128, -74.0060, \"US\", \"United States\", \"North America\"),\n",
    "    (\"London\", 51.5072, -0.1276, \"GB\", \"United Kingdom\", \"Europe\"),\n",
    "    (\"Dubai\", 25.276987, 55.296249, \"AE\", \"United Arab Emirates\", \"Middle East\"),\n",
    "    (\"Singapore\", 1.3521, 103.8198, \"SG\", \"Singapore\", \"Asia\"),\n",
    "    (\"Sydney\", -33.8688, 151.2093, \"AU\", \"Australia\", \"Oceania\"),\n",
    "    (\"Amsterdam\", 52.3676, 4.9041, \"NL\", \"Netherlands\", \"Europe\"),\n",
    "    (\"Tokyo\", 35.6762, 139.6503, \"JP\", \"Japan\", \"Asia\"),\n",
    "    (\"Moscow\", 55.7558, 37.6173, \"RU\", \"Russia\", \"Europe\"),\n",
    "    (\"Hong Kong\", 22.3193, 114.1694, \"HK\", \"Hong Kong\", \"Asia\"),\n",
    "    (\"Shanghai\", 31.2304, 121.4737, \"CN\", \"China\", \"Asia\"),\n",
    "    (\"Paris\", 48.8566, 2.3522, \"FR\", \"France\", \"Europe\"),\n",
    "    (\"San Francisco\", 37.7749, -122.4194, \"US\", \"United States\", \"North America\"),\n",
    "    (\"Los Angeles\", 34.0522, -118.2437, \"US\", \"United States\", \"North America\"),\n",
    "    (\"Frankfurt\", 50.1109, 8.6821, \"DE\", \"Germany\", \"Europe\"),\n",
    "    (\"Zurich\", 47.3769, 8.5417, \"CH\", \"Switzerland\", \"Europe\"),\n",
    "\n",
    "    # --- Indian Cities ---\n",
    "    (\"Mumbai\", 19.0760, 72.8777, \"IN\", \"India\", \"Asia\"),\n",
    "    (\"Delhi\", 28.6139, 77.2090, \"IN\", \"India\", \"Asia\"),\n",
    "    (\"Bengaluru\", 12.9716, 77.5946, \"IN\", \"India\", \"Asia\"),\n",
    "    (\"Hyderabad\", 17.3850, 78.4867, \"IN\", \"India\", \"Asia\"),\n",
    "    (\"Chennai\", 13.0827, 80.2707, \"IN\", \"India\", \"Asia\"),\n",
    "    (\"Kolkata\", 22.5726, 88.3639, \"IN\", \"India\", \"Asia\"),\n",
    "    (\"Pune\", 18.5204, 73.8567, \"IN\", \"India\", \"Asia\"),\n",
    "    (\"Ahmedabad\", 23.0225, 72.5714, \"IN\", \"India\", \"Asia\"),\n",
    "    (\"Jaipur\", 26.9124, 75.7873, \"IN\", \"India\", \"Asia\"),\n",
    "    (\"Surat\", 21.1702, 72.8311, \"IN\", \"India\", \"Asia\")\n",
    "]\n",
    "\n",
    "columns = [\"city_name\", \"latitude\", \"longitude\", \"country_code\", \"country_name\", \"region\"]\n",
    "city_df = spark.createDataFrame(city_data, columns)\n",
    "\n",
    "# Explicitly specify the target columns to avoid column count mismatch\n",
    "(\n",
    "    city_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .saveAsTable(f\"{catalog_name}.{schema_name}.city_master\", \n",
    "                 mode=\"append\")\n",
    ")\n",
    "\n",
    "print(f\"✅ Inserted {city_df.count()} seed cities into city_master.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c125ea67-20c2-4eb6-823b-f55f01021ef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ⚙️ Step 4 – Run Configuration Table\n",
    "\n",
    "The **run_config** table defines pipeline parameters.\n",
    "These help control execution logic dynamically without code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6023ef7-6351-45a8-98a5-6a9477668796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ run_config table created and populated.\n"
     ]
    }
   ],
   "source": [
    "table_name = f\"{catalog_name}.{schema_name}.run_config\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {table_name} (\n",
    "  config_name STRING,\n",
    "  config_value STRING,\n",
    "  updated_ts TIMESTAMP\n",
    ")\n",
    "COMMENT 'Configuration parameters for data pipelines'\n",
    "\"\"\")\n",
    "\n",
    "# Enable default support on the table\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {table_name}\n",
    "SET TBLPROPERTIES('delta.feature.allowColumnDefaults' = 'supported')\n",
    "\"\"\")\n",
    "\n",
    "# Apply default expressions now\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {table_name}\n",
    "ALTER COLUMN updated_ts SET DEFAULT current_timestamp()\n",
    "\"\"\")\n",
    "\n",
    "config_data = [\n",
    "    (\"default_timezone\", \"GMT\"),\n",
    "    (\"historical_start_date\", \"2023-01-01\"),\n",
    "    (\"forecast_horizon_days\", \"7\"),\n",
    "    (\"refresh_frequency_minutes\", \"15\")\n",
    "]\n",
    "\n",
    "columns = [\"config_name\", \"config_value\"]\n",
    "config_df = spark.createDataFrame(config_data, columns)\n",
    "\n",
    "# Explicitly specify the target columns to avoid column count mismatch\n",
    "(\n",
    "    config_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .saveAsTable(f\"{table_name}\", \n",
    "                 mode=\"append\")\n",
    ")\n",
    "\n",
    "print(\"✅ run_config table created and populated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc738e7a-5b42-49bc-bbfb-5be511f15595",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83E\uDDFE Step 5 – Pipeline Log Table\n",
    "\n",
    "The **pipeline_log** table captures audit information for all pipeline runs.  \n",
    "Fields:\n",
    "- `pipeline_name`, `run_id`, `run_type`  \n",
    "- `start_time`, `end_time`, `status`  \n",
    "- `records_processed`, `remarks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f8c91ee-facc-4e16-ade8-95d6a37c4327",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ pipeline_log table created.\n"
     ]
    }
   ],
   "source": [
    "table_name = f\"{catalog_name}.{schema_name}.pipeline_log\"\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {table_name} (\n",
    "  run_id STRING,\n",
    "  pipeline_name STRING,\n",
    "  run_type STRING COMMENT 'HISTORICAL, DAILY, 15MIN', \n",
    "  start_time TIMESTAMP COMMENT 'Run start time', \n",
    "  end_time TIMESTAMP COMMENT 'Run end time', \n",
    "  status STRING COMMENT 'SUCCESS, FAILED, RUNNING',\n",
    "  records_processed BIGINT,\n",
    "  earliest_ts TIMESTAMP COMMENT 'Earliest data timestamp', \n",
    "  latest_ts TIMESTAMP COMMENT 'Latest data timestamp', \n",
    "  triggered_by STRING COMMENT 'Manual, Scheduled, Event',\n",
    "  remarks STRING,\n",
    "  created_ts TIMESTAMP\n",
    ")\n",
    "COMMENT 'Pipeline execution audit and monitoring log'\n",
    "\"\"\")\n",
    "\n",
    "# Enable default support on the table\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {table_name}\n",
    "SET TBLPROPERTIES('delta.feature.allowColumnDefaults' = 'supported')\n",
    "\"\"\")\n",
    "\n",
    "# Apply default expressions now\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {table_name}\n",
    "ALTER COLUMN created_ts SET DEFAULT current_timestamp()\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ pipeline_log table created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffb696b2-8491-4e3d-b6eb-df3392224ce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83D\uDD0D Step 6 – Verification\n",
    "\n",
    "Quick validation to confirm setup success:\n",
    "- All tables exist under `env_data`\n",
    "- City Master contains seed data\n",
    "- Config table shows expected parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c6071a-bcf2-4b38-9253-e1f1c0fe4ed8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCB Tables in schema:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody><tr><td>env_data</td><td>city_master</td><td>false</td></tr><tr><td>env_data</td><td>pipeline_log</td><td>false</td></tr><tr><td>env_data</td><td>run_config</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "env_data",
         "city_master",
         false
        ],
        [
         "env_data",
         "pipeline_log",
         false
        ],
        [
         "env_data",
         "run_config",
         false
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83C\uDF0D City Master sample:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>city_name</th><th>country_name</th><th>active_flag</th></tr></thead><tbody><tr><td>New York</td><td>United States</td><td>true</td></tr><tr><td>London</td><td>United Kingdom</td><td>true</td></tr><tr><td>Dubai</td><td>United Arab Emirates</td><td>true</td></tr><tr><td>Singapore</td><td>Singapore</td><td>true</td></tr><tr><td>Sydney</td><td>Australia</td><td>true</td></tr><tr><td>Amsterdam</td><td>Netherlands</td><td>true</td></tr><tr><td>Tokyo</td><td>Japan</td><td>true</td></tr><tr><td>Moscow</td><td>Russia</td><td>true</td></tr><tr><td>Hong Kong</td><td>Hong Kong</td><td>true</td></tr><tr><td>Shanghai</td><td>China</td><td>true</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "New York",
         "United States",
         true
        ],
        [
         "London",
         "United Kingdom",
         true
        ],
        [
         "Dubai",
         "United Arab Emirates",
         true
        ],
        [
         "Singapore",
         "Singapore",
         true
        ],
        [
         "Sydney",
         "Australia",
         true
        ],
        [
         "Amsterdam",
         "Netherlands",
         true
        ],
        [
         "Tokyo",
         "Japan",
         true
        ],
        [
         "Moscow",
         "Russia",
         true
        ],
        [
         "Hong Kong",
         "Hong Kong",
         true
        ],
        [
         "Shanghai",
         "China",
         true
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "city_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country_name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"CURRENT_DEFAULT\": \"TRUE\"}",
         "name": "active_flag",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Config settings:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>config_name</th><th>config_value</th><th>updated_ts</th></tr></thead><tbody><tr><td>default_timezone</td><td>GMT</td><td>2025-11-11T09:03:39.386Z</td></tr><tr><td>historical_start_date</td><td>2023-01-01</td><td>2025-11-11T09:03:39.386Z</td></tr><tr><td>forecast_horizon_days</td><td>7</td><td>2025-11-11T09:03:39.386Z</td></tr><tr><td>refresh_frequency_minutes</td><td>15</td><td>2025-11-11T09:03:39.386Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "default_timezone",
         "GMT",
         "2025-11-11T09:03:39.386Z"
        ],
        [
         "historical_start_date",
         "2023-01-01",
         "2025-11-11T09:03:39.386Z"
        ],
        [
         "forecast_horizon_days",
         "7",
         "2025-11-11T09:03:39.386Z"
        ],
        [
         "refresh_frequency_minutes",
         "15",
         "2025-11-11T09:03:39.386Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "config_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "config_value",
         "type": "\"string\""
        },
        {
         "metadata": "{\"CURRENT_DEFAULT\": \"current_timestamp()\"}",
         "name": "updated_ts",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\uD83D\uDCCB Tables in schema:\")\n",
    "display(spark.sql(f\"SHOW TABLES IN {catalog_name}.{schema_name}\"))\n",
    "\n",
    "print(\"\uD83C\uDF0D City Master sample:\")\n",
    "display(spark.sql(f\"SELECT city_name, country_name, active_flag FROM {catalog_name}.{schema_name}.city_master LIMIT 10\"))\n",
    "\n",
    "print(\"⚙️ Config settings:\")\n",
    "display(spark.sql(f\"SELECT * FROM {catalog_name}.{schema_name}.run_config\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55caea21-594c-46a1-bbab-27a73590e979",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## \uD83C\uDFC1 Summary\n",
    "\n",
    "**Setup Completed Successfully**\n",
    "\n",
    "| Component | Description |\n",
    "|------------|-------------|\n",
    "| \uD83D\uDCDA Catalog/Schema | `env_catalog.env_data` created |\n",
    "| \uD83D\uDCE6 Volumes | Weather, Pollution, Live Feeds, Checkpoints |\n",
    "| \uD83C\uDF0D City Master | 25 Cities (Insert-only, Versioned) |\n",
    "| ⚙️ Config | Runtime parameters for dynamic control |\n",
    "| \uD83E\uDDFE Pipeline Log | Execution tracking & monitoring |\n",
    "\n",
    "Next → Proceed to **Phase 1: Data Ingestion Pipelines (Historical, Daily, 15-min)**  \n",
    "using the `city_master` and configuration tables as references.\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "00_Setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}